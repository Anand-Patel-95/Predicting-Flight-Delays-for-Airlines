{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a33c042e-408e-4d1a-8eb8-baded24d4056",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### MIDS Spring 22 w261 Final Project\n",
    "# Predicting Departure Delay with Weather and Flight Data \n",
    "### *Zephyr* (team 06) - Anand Patel, Gauri Ganjoo, Matt Lyons, Rumi Nakagawa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "920d1892-081c-4814-b73f-43b7329d095a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 1. Introduction\n",
    "\n",
    "## Background - Trend and Challenges in the Airline Industry \n",
    "\n",
    "According to the report [\"A Review on Flight Delay Prediction\"](https://arxiv.org/abs/1703.06118) and existing flight records, more than 30% of flights are delayed by more than 15 minutes in the United States. In 2017, the Federal Aviation Administration(FAA) estimated that the annual cost of flight delays is $26.6 billion in the United States. Another report argued that half of those costs are covered by passengers. The Wall Street Journal released a ranking of airline services, and flight delays were defined as the two of the most important components of customer satisfaction. It implies that flight delay may affect customer experience with airline services, regardless of whether airline companies caused the delay or not. In fact, the United States Government Accountability Office (GAO) reported that flight delay and cancellation accounted for an average of almost 33% of complaints from air travel passengers for selected airlines. Furthermore, a recent study reported that 85% of the passengers expected to receive rebooking options by airlines. \n",
    "\n",
    "It is an unmissable fact that customer satisfaction and business performance have strong linkage. Delta airlines is the top ranked company with the highest customer satisfaction, and their business growth rate and revenue available per seat mile(RASM) are also the highest among the airlines companies in the US (as of 2020). Scott Kirby, the United president, [announced in 2018](https://blogs.perficient.com/2018/05/14/customer-satisfaction-in-the-airline-industry/) that his airline would focus on improving customer experience through techonology.\n",
    "[McKinsey also reported in the Future of CX (Customer Experience)](https://www.mckinsey.com/business-functions/marketing-and-sales/our-insights/prediction-the-future-of-cx) about the future of boosting data and analytics capabilities for harnessing predictive insights to connect more closely with customers, anticipate behaviors, and identify CX issues and opportunities in real time. \n",
    "\n",
    "   ***“Our focus will be on continuing to improve customer service and expanding United’s network to offer customers more choice.”***\n",
    "   *- January 23, 2018 press release, United president Scott Kirby*\n",
    "\n",
    "  ***“a number of work streams underway to drive a digital transformation over the next few years. We’ve been rolling out a suite of customer self-service tools, which integrate technology in all aspects of travel and improve the customer experience as we work to reduce our costs.”***\n",
    "  *- Robin Hayes, president and CEO of JetBlue Airways*\n",
    "\n",
    "Players in airline industry compete for customer acquisition by leveraging customer experience with techonology. Success in gaining loyal customers will be the decieding factor for this race in the mid/long term. In regards to flight delays, prediction can contribute to improve operations and business performances of airline companies. Improvements in these areas will additionally afford airlines the ability to improve customer experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b7659aa0-6f91-4001-8e87-b15a8e5cf2dc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Business Performance and Customer Satisfaction of Leading Airline Companies\n",
    "\n",
    "|          | Customer Satisfaction | Revenue Available / Seat*Mile | Operating Income 2019 | Passenger 2019 |\n",
    "|----------|:---------------------:|:-----------------------------:|:---------------------:|:--------------:|\n",
    "|    Delta |           1           |           15.4 cent           |         $6.6 B        | 204 M (+6%)    |\n",
    "|   United |           8           |           13.9 cent           |         $4.3 B        |  162 M (+2.6%) |\n",
    "| American |           9           |           14.7 cent           |         $3.1 B        |  215 M (+5.6%) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "27ef147d-f38d-4c09-b745-b595d0b5fb00",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "According to [Delay Impacts Assessment](https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/372619/AC08_tagged.pdf) reported by UK goverment, there are four types of operational cost that airlines cover due to delays. They are fleet cost, crew cost, fuel cost and maintenance cost. They are briefly defined as the additional cost due to longer lead time and reallocation of resource. Having 30% of delays is large enough to burden the business performance of airlines. Adding to this framework, we would like to define a reputation cost as a fifth cost component. Reputation cost links to customer satisfaction, and it can be critical for the company in longer term, because it is the only component that effects directly the topline of the company. If airline companies can anticipate delays, they can potentially reduce cost and loss due to these delays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "090ffbca-156f-43a6-a307-d8bf6c4647cd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Cost component and potential impact of predictions\n",
    "\n",
    "![Cost component and potential impact of predictions](Screen_Shot_2022_04_10_at_5_08_09-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "702c2aca-d952-4eb0-a41e-5bb438338456",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 2. Question Formulation\n",
    "\n",
    "In this section, we would like to articulate the overview of the project. Following the project objective and business use case, research questions and performance measure are discussed. Finally, we would like to show the direction of the initial phase by introducing the past academic literature and our baseline algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2c09da1c-d7d6-4720-ad8d-46f373aee073",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2-1. Who we are\n",
    "We are **Zephyr**, an app company that designs and develops machine learning solutions that lead to improved business performance for clients. In this project, we develop a flight delay prediction app for airline companies. Our key target clients are secondary industry leaders, such as United Airlines. We assume that we have access to flight datasets. If airports have access to better real-time dataset, we will potentially consider building relationships with airports in the longer term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "23ed665a-dfd5-43a7-8d1d-f5ebd27b80f6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2-2. Project Objective\n",
    "\n",
    "In this project, we develop a flight delay prediction app for airline companies that enables them to minimize the loss due to delay and allocate their resources effectively. The goal of this project is to develop a model that predicts whether flights will experience a departure delay two hours before the scheduled time. \n",
    "Our deliverable is a prediction model and designed pipeline, the MVP that tackles the pain of our clients and boosts their key value metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7f43285d-1e01-4bf4-859e-2a3304752f4d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2-3. Our Value - Value Propositions\n",
    "\n",
    "Here are our three value propositions;\n",
    "\n",
    "By knowing the potential delay two hours before the flight,\n",
    "1. Our client can prepare a plan B scenario for resource reallocation\n",
    "2. Our client can prevent a secondary delay due to the lead time for rearrangement\n",
    "3. Our client can improve customer experience by preparing a communication plan in the case of a confirmed delay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e1b04d70-b41f-4e2a-9e75-32a1120648a8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2-4. Business Use Case\n",
    "\n",
    "Airline companies can maximize the business impact of our app through their operational design. As is shown in the figure below, airlines can distinguish the moment of prediction and **confirmation** of delay. Once they recieve the alert of delay two hours before a flight, the operation team can start preparing for re-allocation of various resource such as aircrafts and crew. However, the airline does not need to execute that plan B as soon as the prediction is made. The confirmation and annoucement to stakeholders can be controlled by management. Similarly, customer service can begin preparing for compensation or alternative available options for delayed flight passengers when app users (airline companies) receive the alert of the delay or the users can wait until the appropriate moment to announce the delay to passengers. This \"lag\" enables airline companies to manipulate the business risk better, and they can also incorporate their heuristics to find a sweet spot of the balance of risk and benefit. \n",
    "\n",
    "Passengers can suffer and become stressed out from the uncertainty and long lead time between the confirmation of delay and contact from airlines for alternative solutions. Our app will help our client (airlines) to minimize the lead time by 1. preparing for an alternative solution once they know the potential delay and 2. controlling when to announce the delay to passengers, which will contribute to reduce the stress of passengers. Since airlines can make the decision of when to communicate passegers, this can be achieved with low risk and high reward. We believe the comprehensive operational design improves the reliability and loyality of customers.  \n",
    "\n",
    "#### Business Use Case\n",
    "![Business use case](w261_zephyr_presentation_editing__1_.jpg)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "34c5c7db-65b0-401a-a8ed-bb8b104cefd0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2-5. Datasets\n",
    "There were four datasets that we applied during the project. One was the [Airline On-Time Performance Data](https://www.transtats.bts.gov/Fields.asp?gnoyr_VQ=FGJ) from the Bureau of Transportation Statistics, that contains scheduled and actual departure and arrival times reported by certified U.S. air carriers that account for at least one percent of domestic scheduled passenger revenues. Another was the [Quality Controlled Local Climatological Data (QCLCD) Publication](https://www.ncei.noaa.gov/data/global-hourly/doc/isd-format-document.pdf)\n",
    "from the National Centers for Environmental Information, that contains summaries from major airport weather stations that include a daily account of temperature extremes, degree days, \n",
    "precipitation amounts and winds. The third was a stations dataset that was provided to us that contained geographical information on airport weather stations. \n",
    "The fourth dataset was imported by us to join weather data and flight data. It is an open source [Airport dataset](https://openflights.org/data.html) that contained geographic information on airports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7128aafb-0924-4ff6-b996-070350ac3b65",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2-6. Research questions\n",
    "\n",
    "Based on the dataset, here are the primary research questions we would like to answer throughout the project; \n",
    "\n",
    "#### 1. Target Variable - Which feature do we want to define as a target variable?\n",
    "- There are several options to predict departure delay. If we choose a numeric variable that counts the minute of delay, the project becomes a regression problem. If we choose binary variables, this could be a classification problem. \n",
    "- Note: As is mentioned in the later section, we defined the binary variable `DEP_DEL15` as a target variable. Therefore, the problem we are handling is a classification problem. This variable indicates if a flight is delayed by at least 15 minutes. The rest of the questions are designed based on this decision.\n",
    "\n",
    "#### 2. Join Strategy - How can we make use of weather data for the prediction? What are the limitations to applying weather data for the prediction? How do we want to join data efficiently?\n",
    "- Weather observations are not limited to weather at each airport, and they seem to lack key variables needed for joining and use. We need to consider what kind of transformations we would like to apply to make weather records useful for modeling. \n",
    "- Weather data has a huge amount of features and rows, and finding a reasonable strategy to reduce them is another agenda item to be discussed. \n",
    "\n",
    "#### 3. Feature Engineering - Which features can potentially be dropped and which features should be kept? When is the appropriate step to judge? What kind of feature engineering should we work on?\n",
    "- There were over 130 total variables in the weather data (even more if accounting for multiple report variables of the same type) and around 120 total variables in airline data. The more features we have, the more time and computational power for joining in modeling. Therefore, we need to consider the options of dropping the potential \"noisy\" features in multiple steps. If we drop too much at an initial step, we can expect to reduce the join time, but we may need to go back to the original dataset in the later phase of the modeling if we need more information. If we keep too many features there is increased risk for collinearity. In regards to categorical variables, it will proportionally increase the number of features by the number of nested classes in a feature. Having too many binarized variables makes the dataset sparse, which may affect negatively in modeling due to overfitting and it will certainly increase time spent cleaning columns. We need to consider a strategy on \"when\" and \"what\" we would like to execute. \n",
    "- Secondly, original features may not always express the typical real-world phenomenon when the delay occurs. While this is a machine learning project, we can also think of options to apply heuristics and create features that explain the phenomenon more accurately. This type of work describes feature engineering.\n",
    "- (Generally, adding a fifth new dataset can be another option, but we leave this option because finding a public dataset that has a record of each flight is not realistic.) \n",
    "\n",
    "#### 4. Algorithm Selection - Which machine learning algorithm should be executed in the limited timeline?  \n",
    "- There are various machine learning algorithms. We restricted our investigation to algorithms that can be parallelized to train over the large data, including linear methods and tree methods. This is necessitated by the size of our data and our access to cloud computing clusters. \n",
    "\n",
    "#### 5. Handling Time Series - What are the differences in modeling on time series data? Where are the potentials for data leakage and how do we avoid it?\n",
    "- Knowing that we are handling a time series problem, we will need to apply preprocessing and a modeling technique that avoids data leakage throughout the project. Cross validation and imputation of null values takes this into account. Each feature may have a different appropriate measure to impute the missing value, and we would not be able to apply one single approach to every feature. In addition to cross validation splitting, we also need to discuss the possible best practices on handling advanced time series data in the later phase, such as bootstrapping. Built-in functions for cross validation would not be applicable in this project and more manual applications are needed to apply custom cross validation for our problem.  \n",
    "\n",
    "#### 6. Operational Design in Practice - What kind of operation do we need to design to make our model tolerant and practical? What do we need to take into account to avoid \"castle in the air\"?\n",
    "- In practice, there are potential pitfalls in applying the model that we need to take into account. For example, if we are predicting a flight that's departure time is 12:00 PM, it is not realistic to assume that every real-time weather data and flight observation is updated at 10:00AM. It is more realistic to assume that there are time lags between the current time and the latest accessible data. Once we assume the parameter of such time lags, we are able to change the parameter in the future.  \n",
    "- Another item to consider is the tolerance of the model. Time series data are continually updated, and the system may crash if we assume the first data of the training set is constant. Since we are aiming to provide apps to airline companies, the practical design of the model should be essential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4f676a64-62ac-4c18-ab47-0d4929f10faf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2-7. Metrics\n",
    "\n",
    "We chose the variable as `DEP_DEL15` from the flight dataset as our outcome variable. It is a binary variable that shows whether a flight had been delayed over 15 minutes or not. When the difference between the scheduled departure time and actual departure time is more than 15 minutes, the variable is encoded as 1.\n",
    "\n",
    "Since we are aiming for the \"best\" model through this project, we would like define what we consider the \"best\". When a machine learning model is applied to the decision making process, minimizing the business risk typically matters more than predicting accuracy, because realistically we would not be able to develop a perfectly accurate model. There are always trade-offs between benefit and risk of incorrect predictions, and the client wants to understand the risk precisely.\n",
    "\n",
    "Based on the above perspective, we applied an F-score as a performance measure in this project. F-score is a hybrid measure of recall and precision. In other words, it tracks both the false positive(FP) rate and false negative(FN) rate. Minimizing business risk is recontextualized as minimizing FP and FN, because our client will incur more cost through such predictions. At a higher level, it is also recognized as a measure to track \"accuracy\". We can manipulate the proportion of recall and precision in F-score, based on business context or domain knowledge in the industry. FP describes the case that is predicted as delay, but is actually no-delay. FN describes the case that is predicted as no delay, but is actually delay.\n",
    "\n",
    "Going back to the former section, we had five cost components(Figure 1) in case of delay. Impact of FP and FN should differ in each cost component, so as the potential quantity of the impact. We defined the details of the F-score through cost analysis, as is shown in Figure 1.\n",
    "\n",
    "#### Figure 1. Business Risk Ananlysis of Cost Components\n",
    "![metrics](w261_zephyr_presentation_editing-2.jpg)\n",
    "\n",
    "Here, we would like to analyze how each component can be affected by FP and FN. We also added weights to each component, based on the assumed quantity of business impact of each component.\n",
    "\n",
    "1. **Reputation cost**\n",
    "    - **FP**: Proactive airline may want to communicate delays with customers ahead of time. Telling customers their flight will be delayed when it is not, may lead to flyers arriving late and missing their flight. Bad for reputation.\n",
    "    - **FN**: The airline did not take any measures for a delayed flight, and are forced to communicate to customers that their flights are delayed and there are no advanced measures in place for dealing with this. Bad for reputation, this airline would be seen as unprepared.\n",
    "    - **FP: FN = 50:50, weight: 0.4**\n",
    "2. **Fleet cost**\n",
    "    - **FP**: High cost of relocating another airplane, crew, time from the airline’s network to carry these people later in the event of a delay, when it turns out to be unnecessary is a heavy planning effort to do and undo when FP’s occur.\n",
    "    - **FN**: Chain necessitates of rebooking/reimbursing/accommodating customers\n",
    "    - **FP : FN = 50:50, weight: 0.15**\n",
    "3. **Crew cost**(FP: FN = 60:40, weight: 0.15)\n",
    "    - **FP**: Paying crew overtime and dedicating crew for expected delayed flights becomes a problem if there is no delay and the airline needs to pay for crew time.\n",
    "    - **FN**: The people working the delayed flights find out the time of the delay and might become an issue if work schedules are irregular.\n",
    "    - **FP : FN = 60:40, weight: 0.15**\n",
    "4. **Fuel cost**(FP: FN = 50:50, weight: 0.15)\n",
    "    - **FP & FN**: Regardless of having FP or FN, same quantity are consumed to loiter. \n",
    "    - **FP : FN = 50:50, weight: 0.15**\n",
    "5. **Maintenance cost**(FP: FN = 50:50, weight: 0.15)\n",
    "    - **FP & FN**: Regardless of having FP or FN, same amount of equipment usage used to treat airplane on the runway\n",
    "    - **FP : FN = 50:50, weight: 0.15**\n",
    "    \n",
    "Customer reputation is the only component that affect directly to the topline, therefore we weighted as 40%. As a result the proportion of Precision and Recall became nearly 50:50, therefore we applied **F1 score** as our performance measure. Since both false positive results and false negative results would potentially cost the airlines, we will be measuring our models F1 score to minimize both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4ce84e7f-2139-42f2-b07e-6c7ca522f239",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2-8. Overview of Approach and Baseline\n",
    "\n",
    "Flight delay prediction has been a long-run study for more than twenty years. The latest [review](https://arxiv.org/abs/1703.06118), which was published in 2018, introduces the related studies in vairous scope, agenda and approaches(Figure 2).\n",
    "\n",
    "\n",
    "#### Figure 2. Various Approaches of Past Studies with Flight Delay(Quotation: [A Review on Flight Delay Prediction](https://arxiv.org/abs/1703.06118) )\n",
    "![Figure 4: Categories of methods used to model the flight delay prediction](Screen_Shot_2022_04_10_at_14_55_34.png)\n",
    "\n",
    "\n",
    "\n",
    "Application of deep learning has been the emerging trend in recent years, where as various machine learning algorithms have also been continuously studied. Since the flight data contains both numerical variable and binary variable, both regresssion and clssification problem is studied, while the review reported that commonly used algorithm such K-NN, SVM, fuzzy logic and random forest are mainly used for classification problem in flight delay. \n",
    "Here are some examples of the recent past studies;  \n",
    "\n",
    "\n",
    "- [Predicting Flight Delay Risk Using a Random Forest Classifier Based on Air Traffic Scenarios and Environmental Conditions](https://ieeexplore.ieee.org/document/9256474)\n",
    "- [Flight Delay Prediction Using Machine Learning Algorithm XGBoost](https://www.researchgate.net/profile/Subhani-Shaik-6/publication/344227817_Flight_Delay_Prediction_Using_Machine_Learning_Algorithm_XGBoost/links/5f5e243c92851c0789635a7d/Flight-Delay-Prediction-Using-Machine-Learning-Algorithm-XGBoost.pdf)\n",
    "- [Prediction of weather-induced airline delays based on machine learning algorithms](https://ieeexplore.ieee.org/abstract/document/7777956)\n",
    "- [Characterization and prediction of air traffic delays](https://www.sciencedirect.com/science/article/pii/S0968090X14001041)\n",
    "\n",
    "Tree models, represented by random forest, are observed as one of the most established approaches. Recent studies of tree models include wide variety of agenda, such as traffic delay, weather-related delay or applying regression models. Since we indentified our problem as a classfication problem, comparing the performance of multiple tree models could be a steady option to gain the robust performance in the limited timeline. As a technical novelty, we also plan to create various flight related features, including the ones created through graph theory.\n",
    "\n",
    "While our primary focus will be to execute tree models, we would like to choose logistic regression as a baseline model. There are three reasons for this choice. First, it is one of the simplest but the most powerful classification algorithm which has been applied in practice across various industries. Secondly, train speed of the algorithm is fast. Since the fight delay is completely new agenda for our company, logistic regression helps us understand the overview and characteristics of the dataset and model. Finally, we are able to apply regularization in logistic regression. Not only the advantage to avoid overfitting, but also the feature selection capability of Lasso regularization helps us drop insignificant features. Selected features through L1 regularization can also be the potential significant features in different algorithm, and we could sequentially make use of the insight to the next step. Having large variety of categorical variables in the original dataset implies the future challenge of sparsity of the dataset. Result of the logistic regression can be the good guildline to identify the direction of the further step of the study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "10156e60-9da2-4a95-9a85-9a31fa6260f2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 3. EDA & Discussion of Challenges\n",
    "\n",
    "In this section we would like to discuss the execution and that led to determine the direction of modeling in the later phase. We also discuss how we joined the multiple dataset, based on the findings in the earlier sections of EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b8c366b9-41d9-4baa-bae1-9382083b2840",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 3-1. Flight EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a062b547-b873-40a9-b30f-38bbc9761380",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 3-1-1. Pre-Join\n",
    "\n",
    "Pre-join EDA was done in the first two weeks, mainly with 6 months data. While there were limitation in the variety of records, there were still valuable findings and insight that led to helpe identifying the direction of the later phase. In shifitng the step to join full data, dupliated records became another inevitabld agenda . \n",
    "\n",
    "#### What is done here\n",
    "**1. Observed characteristics of each feature (6 months data)**\n",
    "- Each feature was studied in  [worksheet of original features](https://docs.google.com/spreadsheets/d/1wFra4oo6wdiy23kbUswi-f44yeCXxId_3uHuLfTqSvs/edit?usp=sharing) and [data Profile](https://docs.google.com/document/d/1ZEj5TNxJNUzT-8MExU399fN4z5IQU5HDOSv-AgbAvmA/edit?usp=sharing). Primary focus was to study each their attribute(Ex. numerical or categorical), distribution, missing value, and then to identify the potential useful features as well as potential features to drop before join.\n",
    "\n",
    "**2. Simple Analysis Based on Hypotheses (6 months data)**\n",
    "- Based on the interests and ideas that came up through the above observation, quick analysis was done in with 6 months data in [EDA(pre-join)](EDA_rumi2_Airlines.ipynb). The findings here led to the feature engineering in the later phase.\n",
    "\n",
    "**3. Preparation for Join(full data)**\n",
    "- Once we got access to full data, validation was done to see if the same trend were observed in the 5-years data. \n",
    "- Assuming the use of weather data in each airport for prediction in the later phase, there were few questions to consider in the initial phase of the project. One was to identify which feature was appropriate to apply as a key, and also to observe whether there were duplicates in recored that could harm or led to error in combining dataset. We would like to discuss briefly about the findings here, and details are discussed in the later section. \n",
    "\n",
    "--- \n",
    "\n",
    "#### Key Findings \n",
    "\n",
    "#### 1. Observations of each feature\n",
    "\n",
    "  - Reference : [Worksheet of each features](https://docs.google.com/spreadsheets/d/1wFra4oo6wdiy23kbUswi-f44yeCXxId_3uHuLfTqSvs/edit?usp=sharing), [Data profile](https://docs.google.com/document/d/1ZEj5TNxJNUzT-8MExU399fN4z5IQU5HDOSv-AgbAvmA/edit?usp=sharing)\n",
    "  - There are around 110 original features, and more than 30% of the features are related to diverted flights.\n",
    "  - More than 99.99% of the values were missing in some features. If the definition of some of those features are not likely to related to flight delay, they could be possibly dropped before join to reduce the run time of joing. \n",
    "  - Not only features such as `DEP_DELAY`, `DEP_DELAY_NEW` but there were also some features that were the family of the outcome variable.  \n",
    "    - Example: `CARRIER DELAY`, `WEATHER DELAY`,`NAS_DELAY`, `SECURITY DELAY`, `LATE_AIRCRAFT_DELAY`\n",
    "  - Some features seemed to be worth analyzing the potential correlation between flight delay(`DEP_DEL15`) to brush up better hypotheses.\n",
    "    - Example: `TAIL_NUM`, `OP_CARRIER_FL_NUM`, `ORIGIN` & `DEST` etc\n",
    "  \n",
    "#### 2. Simple analysis across multiple features\n",
    "  - 6 Months dataset contained various destinations all over the United States, and it did not include any international flights(destination airports) in the dataset. \n",
    "  - There were multipe features that worked similarly. We may choose one in the later phase.\n",
    "    - Example: `OP_UNIQUE_CARRIER` `OP_CARRIER_AIRLINE_ID` ` OP_CARRIER` \n",
    "  - There are more than 7,000 null values in the target variable. *6month dataset\n",
    "\n",
    "**Figure 3**\n",
    "\n",
    "\n",
    "![similarity + null in target variable](Screen_Shot_2022_04_11_at_11_22_27.png)\n",
    "  \n",
    "---\n",
    "  - Potential correlation between flight delay by month/season was observed, and trend seemed to be different between airports(`ORD` vs `ATL`).\n",
    "\n",
    "**Figure 4**\n",
    "\n",
    "\n",
    "![month/season and dep_del15](Screen_Shot_2022_04_11_at_11_32_03-1.png)\n",
    "\n",
    "--- \n",
    "- There seemed to be a trend between destinations and flight delay, since specific route had larger/smaller ratio of delay out of total counts of flights during six months. Figure 5 only focuses on `ORD` as an `ORIGIN`, however, similar trend was also found in `ATL` as a `ORIGIN`. \n",
    "- The destinations that had more flights were more likely to have larger absolute number of delay, while there seemed to be less correlation between the size of airport and the ratio of the fight delay. \n",
    "\n",
    "**Figure 5**\n",
    "\n",
    "\n",
    "![destination and delay](Screen_Shot_2022_04_11_at_11_45_39.png)\n",
    "\n",
    "\n",
    "**Figure 6**\n",
    "\n",
    "\n",
    "![month/season and dep_del15](Screen_Shot_2022_04_11_at_11_46_47.png)\n",
    "\n",
    "--- \n",
    "\n",
    "- Potential correlation between the flight number and target variable were also observed. While the total number of flights varied by carriers, flight number with top 5 counts of flights in 6 months were chosen, and the ration of flight delay of the 5 were compared with average flight delay ratio. Top 5 flight number tend to have larger ratio of flight delay than the average of the whole. \n",
    "**Figure 7**\n",
    "\n",
    "![destination and delay](Screen_Shot_2022_04_11_at_11_53_07.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7932b862-10ef-4dbc-ae3b-6eca9ae12199",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 3. Preparation for Join\n",
    "\n",
    "- As with the 6 months data, only domestic flights were found in the full data. Through this finding, the scope was confirmed as domestic flights in the project. Similarly, potential correlation between tail number and target variable, or route and target variable were observed in the 5-years data.(Figure 9)\n",
    "- We found a number of true duplicates (flights with the same carrier, flight number, origin, destination, and departure time) which we needed to cull.\n",
    "- We performed a small join as a precursor to the full join and used the Haversine distance along the Earth's surface to determine the closest weather station to each airport. As shown below, the weather stations were generally close enough to be acceptable (Figure 8).\n",
    "- We did some examination of the times for departure and landing. Both times were given in local time and needed to be converted to UTC for the join to work properly (detailed more under Joining Datasets).\n",
    "  - Relatedly, we examined records for flights scheduled to depart before midnight but which departed after midnight\n",
    "  - We determined that the date given is for the *scheduled* departure date.\n",
    "  - This will need to be carefully taken into account for any future features which may involve the arrival datetime.\n",
    "\n",
    "\n",
    "#### Figure 8\n",
    "![Station](Screen_Shot_2022_04_11_at_7_39_24.png)\n",
    "\n",
    "\n",
    "#### Figure 9\n",
    "![tail number, route and delay](Screen_Shot_2022_04_11_at_7_40_30.png)  \n",
    "![sample code1](Screen_Shot_2022_04_11_at_11_05_12.png)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### Insights for the Next step  \n",
    "- Including features that related to diverted flights might limit the use of other features that related to airports, therefore features of diverted flights could be possibly dropped in the later phase. \n",
    "- Potential correlation between target variable and tail number/flight number/route can be one of the key discussions in the later phase of the project. Specifically, they are all categorical variables which contain large number of classes. Having large amount of sparse features could make the performance of the model worse, hence alternative options (ex. creating features) could be the necessary discussion in the next phase. \n",
    "- We will need to carefully examine our record counts before and after the join to ensure we have not pulled in null values or duplicate records.\n",
    "- Delays can cause havok with true versus scheduled arrival date/times. The datetimes will need to be handled carefully both to avoid data leakage and to ensure we are looking at the correct time window.\n",
    "\n",
    "--- \n",
    "\n",
    "#### References  \n",
    "- [Worksheet of original features](https://docs.google.com/spreadsheets/d/1wFra4oo6wdiy23kbUswi-f44yeCXxId_3uHuLfTqSvs/edit?usp=sharing)\n",
    "- [EDA_Visualization(pre-join)](EDA_rumi2_Airlines.ipynb)\n",
    "- [EDA_Visualization(post-join)](EDA_rumi4_POSTJOIN_rumi.ipynb)\n",
    "- [Data Profile](https://docs.google.com/document/d/1ZEj5TNxJNUzT-8MExU399fN4z5IQU5HDOSv-AgbAvmA/edit?usp=sharing)\n",
    "- [Convert to UTC time](EDA_matt.ipynb)\n",
    "- [EDA_sub_notebook(pre-notebook)](EDA_rumi1_join_airline_stationID.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2c2710fe-263d-4216-87c2-7e0075cb2e5c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 3-1-2. Post-Join\n",
    "After joining our data, we produced 31,080,934 flight records. Our feature engineering section walks through how we processed the variables in this dataset into features. This process took into consideration for the columns; the number of missing values, if each column would be data leakage to include as a feature, the meaning of each variable and if it made sense to use these as predictors, how to encode information about columns of interest in the context of flights (ex. tail numbers track planes throughout the day). \n",
    "\n",
    "Through this process, we produced several new features while also including valid features that would be known at the time of prediction (ex. weather features, airport, carrier, route encodings). Our EDA gives insight into these features.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### What we have done\n",
    "- Also for Post-Join EDA, we produced 5 correlation heatmaps for subsets of all 78 of our features and the outcome variable `DEP_DEL15`. The table would show us the Pearson correlation between a feature and the outcome. The bottom row of each map gives the correlation with the outcome. The correlation heatmaps are ordered and broken up as follows:\n",
    "\n",
    "  1) Heatmap 1: Tier 1 features (The most important features determined by our Random Forest Feature Selection later)\n",
    "  2) Heatmap 2: First half of Tier 2 features (The 2nd most important features determined by our Random Forest Feature Selection later). Just the first, more important, half of Tier 2 features.\n",
    "  3) Heatmap 3: Second half of Tier 2 features (The 2nd most important features determined by our Random Forest Feature Selection later). Just the second, less important, half of Tier 2 features.\n",
    "  4) Heatmap 4: First half of Untiered features (The least important features determined by our Random Forest Feature Selection later). Just the first, more important, half of Untiered features.\n",
    "  5) Heatmap 5: Second half of Untiered features (The least important features determined by our Random Forest Feature Selection later). Just the second, less important, half of Untiered features.\n",
    "\n",
    "---\n",
    "\n",
    "#### Findings \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "--- \n",
    "\n",
    "\n",
    "#### Figure 10\n",
    "\n",
    "![correlation heatmap: tier 1](corr_t1.png)\n",
    "\n",
    "The heatmap above shows the correlation for each Tier 1 feature with each other and, more importantly, with our outcome variable `DEP_DEL15` in the bottom row. The strongest correlated negative feature with the outcome here was `plane_is_here`, a status based on the tail number indicating of this flight's plane is already at the airport, with a negative correlation of -0.48. If the plane is already at the airport, the chance of this flight being delayed is less because the turnaround for getting the plane flying should be made more accessible. The strongest correlated positive feature with the outcome here was `avg_hourly_delay_24hr`, the proportion of delays by hour within the last 24 hours of the flight, with a correlation of -0.48. If the plane is already at the airport, the chance of this flight being delayed is less because the turnaround for getting the plane flying should be made mor accessible. Most Tier 1 features have a correlation above 0.1 or below -0.1, showing that these may be good predictors of `DEP_DEL15` in a machine learning model. The exception to this was `OD_avg_DEP_DEL15`, the proportion of delayed flights along this route from the quarter before our flight, since its correlation was 0.097.\n",
    "\n",
    "#### Figure 11\n",
    "![correlation heatmap: tier 2, first half](corr_t2_1.png)\n",
    "\n",
    "The heatmap above shows the correlation for the first half of the Tier 2 features with each other and with our outcome variable `DEP_DEL15` in the bottom row. The strongest positively correlated feature with the outcome here was `avg_OD_delay_min_24hr`, the average delay in minutes of flights flying this route in the 24 hours before our flight's prediction time, with a correlation of 0.21. A longer delay of recent flights along this route may indicate that our current flight will be delayed. The strongest negatively correlated feature with the outcome here was `OD_delay_ordinal`, the ranking of this route based on its proportion of delayed flights last quarter, with a correlation of -0.31. A higher ranking means this route is less delay prone, and this might indicate that our current flight along it will not be delayed. Most top Tier 2 features have a correlation above 0.1 or below -0.1, showing that these may be good predictors of `DEP_DEL15` in a machine learning model. There were more exceptions now; namely the weather features.\n",
    "\n",
    "#### Figure 12\n",
    "![correlation heatmap: tier 2, second half](corr_t2_2-1.png)\n",
    "\n",
    "The heatmap above shows the correlation for the second half of the Tier 2 features with each other and with our outcome variable `DEP_DEL15` in the bottom row. The strongest positively correlated feature with the outcome here was `origin_avg_DEP_DEL15`, the average proportion of delayed flights from this airport last quarter, with a correlation of 0.078. Higher delay rate at this airport last quarter may indicate that the current flight is likely to be delayed, but the correlation is very small. The strongest negatively correlated feature with the outcome here was sky ceiling height, with a correlation of -0.2. A higher ceiling height for clouds might indicate that our current flight will not be delayed. Only half of the bottom Tier 2 features have a correlation above 0.1 or below -0.1 and not by much, these features might not make good predictors of `DEP_DEL15` in a machine learning model. \n",
    "\n",
    "#### Figure 13\n",
    "![correlation heatmap: no tier, first half](corr_no_tier_1.png)\n",
    "\n",
    "The heatmap above shows the correlation for the first half of the Untiered features with each other and with our outcome variable `DEP_DEL15` in the bottom row. The strongest positively correlated feature with the outcome here was wind gust speed with a correlation of 0.042. The correlation is very small. The strongest negatively correlated feature with the outcome here was an indicator for the flight being in autumn, with a correlation of -0.12. Perhaps autumn flights are less likely to be delayed. Only 2 of the bottom Tier 2 features have a correlation above 0.1 or below -0.1 and not by much, these features might not make good predictors of `DEP_DEL15` in a machine learning model. \n",
    "\n",
    "#### Figure 14\n",
    "![correlation heatmap: no tier, first half](corr_no_tier_2.png)\n",
    "\n",
    "The heatmap above shows the correlation for the second half of the Untiered features with each other and with our outcome variable `DEP_DEL15` in the bottom row. The strongest positively correlated feature with the outcome here was `OD_avg_DEP_DELAY_NEW`, the average delay in minutes along this route last quarter, with a correlation of 0.14. If the delay along this route was long last quarter, this may indicate a delayed flight along it. The strongest negatively correlated feature with the outcome here was a tie between the air pressure and altimeter rate, with a correlation of -0.19. Again, most Tier 2 features do not have a correlation above 0.1 or below -0.1, so most of these features might not make good predictors of `DEP_DEL15` in a machine learning model. \n",
    "\n",
    "---\n",
    "\n",
    "#### Insights for the Next step \n",
    "- The next steps include a possible implementation of feature selection for the features that make the best predictors, and an assessment of those through machine learning models. We also must devise a modeling and evaluation framework.\n",
    "\n",
    "#### Challenges\n",
    "- Most of the flight data in its raw form is not usable as features for lack of relevance or lack of availability to the app at prediction time. New features must be made and EDA happens after mostly.\n",
    "- Weather data needed significant processing before EDA could be done on its relationship with our outcome.\n",
    "\n",
    "Associated Notebook: [Post Join Feature Correlation Heatmaps](Feature_selection_correlation_only_rumi_anand.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6999178f-8fcc-404b-9aac-5ea232db9a92",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 3-2. Weather EDA\n",
    "\n",
    "--- \n",
    "### 1. Pre-Join\n",
    "\n",
    "#### What we have done\n",
    "1. Our first action with the weather data was to downsize the dataset size, by ~90%, by filtering for rows where the station id of the weather station would match a station id from our `nearest_stations` data (where we kept stations that match airports across the world). Our resulting table, `df_weather_filtered`, had ~81.1 Million rows. We perfomed EDA plots on the columns of `df_weather_filtered` using the data profile functionality in pyspark. This let us [generate distributions](https://docs.google.com/document/d/1D0hsR62ME1TEVg642ifIe-k4wGhnD-Z82tSkpbFte-Y/edit) of values for all columns in weather. \n",
    "\n",
    "2. After dropping the trivial variables(ex: station city), we went through the weather data dictionary, which consisted of over 130 variables, and used our team member Anand's background experience in aerospace engineering/aviation to make intuitive decisions about which features to outright drop in the initial phase due to low expected correlation with aircraft flights. This was crucial for downselecting weather variables, since many would be difficult to unpack and prepare as features. How we chose to work with each weather variable is documented in our [Google Sheet](https://docs.google.com/spreadsheets/d/1eNBuO1KSAi8oTsGigcdBJ34J2H9PsogzR7o8gPKl2-M/edit#gid=1474431792), where we kept track of each of these features meant, how to deal with remaining missing values (`\"\"`, \"9999**\", Nulls), and any derived features that we needed to generate (including unnesting, dealing with categoricals, etc).\n",
    "\n",
    "#### Findings \n",
    "\n",
    "---\n",
    "\n",
    "#### Figure 15\n",
    "\n",
    "![Prejoin EDA](Screen_Shot_2022_04_10_at_8_04_16_PM.png)\n",
    "\n",
    "1. **Trival Features**\n",
    "    - If we saw that a variable had only unique value from these plots, then we would drop the variable as it could not add information to our model.\n",
    "    - Example: \n",
    "      - The variable `KF1` each had 1 unique value for the entire dataset, so we decided to drop them.\n",
    "\n",
    "2. **Empty Features**\n",
    "    - We also saw that many variables returned empty strings(i.e. `\"\"`) for most of the rows. For those cases, despite the data profiles giving 0% missing, the empty strings were essentially missing values.\n",
    "    - Example: \n",
    "      - `AZ2` had two unique values, but its most common value was \"\", which meant that there was no observation recorded and was, in practice, a missing value.\n",
    "      \n",
    "3. **Repeated Features**\n",
    "    - Many features had multiple identical variables and our philosophy was generally to keep the variable that had fewer missing values since these features, on a case-by-case basis, represented different quantities. If we did keep multiple identical variables, they would be encoded together, which we will explain further down.\n",
    "    - Example:\n",
    "      - The variable `MV1` recorded a weather occurrence within 5 to 10 miles of a station. `MV2` recorded a second weather occurrence. There were up to seven weather occurrences that could have been recorded, all with an `MV` variable. However we found that after `MV1`, the other occurrences were rarely reported, especially more than two, so we opted to drop the other `MV` variables.\n",
    "\n",
    "4. **Numeric Features**\n",
    "    - Some weather variables were numeric. For those variables, we would impute missing values before testing for the model.\n",
    "    - Example:\n",
    "      - `OC1` recorded the wind gust speed. \n",
    "      \n",
    "5. **Categorical Features**\n",
    "    -  The majority of the weather data were nonbinary categorical data, and were stored as nested strings. Some weather variables of interest were categorical, with almost 100 options. After deciding which ones to keep, we engineered a few new binary indicators for presence of extreme weather hindering flights (ex. thunder, tornado, fog, hail, etc.) by checking if the value of the column is one of the categories associated with this binary indicator. We could populate missing values from the original variable, or different values from the category of interest, using 0's.\n",
    "\n",
    "    - Example: \n",
    "      - `AW1` had 100 possible categorical values. There were too many to one-hot encode, but we did not want to lose the information all together.\n",
    "\n",
    "5. **Takeaway: Feature Imputation Needs**\n",
    "- A key part of the decision making process for keeping and dropping variables included how much data was missing. Generally, we kept variables that had over 50% populated values. Even if a variable has 99% populated values, we looked on case-by-case to see if there was a logical way to populate missing values. For example, let us consider a feature measuring inches of snow that might have 99% missing values. The data dictionary indicated that the numerical value nested inside gives the mm of snow on the ground. It can be safe to assume that the default scenario might be no snow on the ground, and snow would only impact flights if there was a large amount on the ground. In that case, we could populate missing values with 0 for no mm of snow. For another example, let us consider a feature measuring pressure. Since pressure is usually given as nonzero, we could decide to populate missing values with an average. We will discuss this process further down. We should also note that we did not consider imputing missing values like this a data leakage error because weather is not affected by flights or flight delays and is generally similar year to year\n",
    "\n",
    "#### Insights for the Next step  \n",
    "- We will need to make a plan for imputing missing values, both categorical and numeric.\n",
    "- Our next step is to look at the correlation between varibales to see their potentials as predictors.\n",
    "\n",
    "### 2. Post-Join\n",
    "\n",
    "\n",
    "#### What we have done\n",
    "- After cleaning, unnesting, converting and joining the weather data, we were able to look more at the correlations between variables and better understand their potential for predicting delays. We also imputed null/empty values, which we discuss further down/  \n",
    "\n",
    "\n",
    "#### Findings \n",
    "---\n",
    "#### Figure 16\n",
    "![Weather corr matrix](image1-1.png)\n",
    "\n",
    "- Looking a confusion matrix for a subset of the weather variables, we can see that there is a sizable amount of  collinearity between variables.\n",
    "\n",
    "#### Figure 17\n",
    "![Weather corr matrix](Screen_Shot_2022_04_08_at_10_44_47_PM-3.png)\n",
    "- Looking at a second confusion matrix, we can see that there appears to be less correlation between the DEP_DEL15 variable and the weather variables compared to each other. Note that the same weather variables are not used in each matrix, but they do still illustrate the relationships.\n",
    "\n",
    "#### Insights for the Next step \n",
    "- Looking at our correlation tables, we can start to guess that we will need to frop more weather variables some are highly correlated with others. \n",
    "- We also get our first hint that the weather data may not be a reliable predictor on its own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6767777f-8102-4446-820f-f5961516c290",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 3-3. Weather Stations\n",
    "- [Link to Matt's join to get Nearest Weather Stations](EDA_matt.ipynb)\n",
    "- [Rumi's EDA 1 on weather stations & airports](EDA_rumi1_join_airline_stationID.ipynb)\n",
    "\n",
    "#### Findings and team decisions\n",
    "- The weather stations table was joined with an external dataset of global airports ([OpenFlights](https://openflights.org/data.html)) to keep just the weather station closest to an airport. We found this by looking up the latitude and longitudes of weather stations and airports, using the external dataset, and calculating the Haversine's distance between them.\n",
    "  - \\\\(d = 2 r \\arcsin{(\\sqrt{ \\sin^2{\\frac{\\phi_2 - \\phi_1}{2}} + \\cos{\\phi_1} * \\cos{\\phi_2} * \\sin^2{\\frac{\\lambda_2 - \\lambda_1}{2}}})}\\\\)\n",
    "  - Where \\\\(\\phi_1, \\phi_2\\\\) are the latitude of point 1 and latitude of point 2.\n",
    "  - Where \\\\(\\lambda_1, \\lambda_2\\\\) are the longitude of point 1 and longitude of point 2.\n",
    "- We restrict our scope to US domestic airports, since most weather stations were found to be US-based in the EDA and weather readings from the nearest US weather station do not make sense for international airports. \n",
    "- We produce an intermediate table of every airport of interest, with its nearest weather station. We join flights on the left with this table applying IATA as a key. We produce a nearest weather station to every flight record.\n",
    "\n",
    "#### Challenges\n",
    "- We expect extra rows occuring because some weather stations took two observations at the same time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f9ec54e4-7b84-4f15-9c34-9b91646b460c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Joining Datasets\n",
    "---\n",
    "#### Figure 18\n",
    "![data_flow](data_flow.png)\n",
    "\n",
    "### Preparation\n",
    "- Imported [OpenFlights](https://openflights.org/data.html) data for airport latitude/longitude, elevation, and [IANA/Olson timezone](https://en.wikipedia.org/wiki/Tz_database) for each airport\n",
    "  - Joined to Flights data on IATA code\n",
    "- Removed neighbors from station data (filter where Station ID == Neighbor ID)\n",
    "  - Computed Haversine distance from each airport (IATA code) to each weather station using latitude/longitude (as mentioned above)\n",
    "  - Selected nearest station to each airport\n",
    "  - Joined to flights data on IATA code for both origin and destination airports\n",
    "- Converted all CRS departure date/times into timestamps and converted to UTC using timezone data\n",
    "- Created columns for the lookback window (START and END)\n",
    "  - These run from 4 hours before the flight to 2 hours before the flight\n",
    "- Filtered weather data to include only reports from stations close to an airport\n",
    "\n",
    "### Basic Strategy\n",
    "- Created Temp View as a \"bridge\" between Flights and Weather\n",
    "  - Chose all reports for each station within a lookback window of a departing flight from the associated airport(s)\n",
    "    - We initially tried 4 hours, but this was unnecessarily large\n",
    "    - A lookback window of 2 hours was sufficient to capture reports for all flights in the 6 month dataset\n",
    "  - Chose the max datetime (MAXDATE) for each combination of END time of window and station ID\n",
    "- Joined Flights to Weather, selecting only the observations with a matching Station ID and MAXDATE\n",
    "\n",
    "#### Figure 19\n",
    "![data_flow](join_diagram.png)\n",
    "\n",
    "### 6M Test Join (ORD and ATL only)\n",
    "- [Join Notebook](pipeline_6m_data_2_airports_only.ipynb)\n",
    "- Filtered weather to dates prior to 01JUL2015\n",
    "- Filtered weather data to include only the two stations closest to ORD and ATL \n",
    "- Upon analyzing these data, we discovered 41 more rows here than there were flights\n",
    "  - These correspond to FM-12 and FM-16 reports from the same station in the same minute\n",
    "  - We determined that FM-16 tends to contain more consistent/valuable information\n",
    "  - When there is a duplicate, we will choose FM-16 first\n",
    "  - We opted to do this filtering after the join instead of during\n",
    "  - These amount to about 0.012% \"duplicates\"\n",
    "- The join itself was very quick (<1s) but the writing took about 30 minutes (this varies tremendously depending on cluster usage)\n",
    "- The join resulted in about 338,000 rows\n",
    "\n",
    "### 6M Test Join 2 (all airports)\n",
    "- [Join Notebook](pipeline_6m_data.ipynb)\n",
    "- Filtered weather and flights to dates prior to 01JUL2015\n",
    "- Filtered weather data to include only reports from stations closest to an origin airport\n",
    "- As above, we noted many (2885) pseudo-duplicates (reports filed in the same minute from the same station)\n",
    "  - This amounted to about 0.052% \"duplicates\"\n",
    "  - Some of these were true duplicates and can easily be removed\n",
    "  - Others will require a preference order based on report type as we did for FM-16 and FM-12\n",
    "- The join itself was, again, quick to run (<1s) but writing to blob took just over 90 minutes \n",
    "  - With a more powerful cluster, we believe this will scale well enough for writing the full dataset\n",
    "- The join resulted in about 2.9 million rows\n",
    "\n",
    "### Full Join\n",
    "- [Join Notebook](pipeline_full_data_join.ipynb)\n",
    "- Started with all flights and weather data\n",
    "- Filtered weather data to include only stations closest to origin airports\n",
    "- Upon analyzing these data, we discovered significantly more rows here than there were flights\n",
    "  - In this case, hard-coding report types would not work well, so instead we chose the report with the most non-null rows\n",
    "- The join and write took 2 hours, 5 minutes\n",
    "- **Our number of flight records post join is 31,080,934.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3c56b1de-6650-4844-8566-5913c4af5264",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 4. Feature Engineering \n",
    "\n",
    "#### Figure 20\n",
    "![feature engineering](w261_zephyr_presentation_editing__3_.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7783fd99-5539-468a-830b-cc0bc0109dd9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Derived Flight Features\n",
    "Notebook Links:\n",
    "- [Notebook: Feature Engineering 1](FeatEng_matt1_time_and_loc_full.ipynb)\n",
    "- [Notebook: Feature Engineering 2](FeatEng_anand2_Cleaning_Flights_Joined_full.ipynb)\n",
    "- [Notebook: Feature Engineering 3](FeatEng_anand3_Cleaning_Flights_Joined_full.ipynb)\n",
    "\n",
    "### Features from Time/Date/Location Metadata\n",
    "\n",
    "Associated Notebook: [Feature Engineering 1 Notebook](FeatEng_matt1_time_and_loc_full.ipynb)\n",
    "\n",
    "We derived several features based on the time of day, date/part of the year, and region of the origin airport. Our reasoning was that there are several levels of seasonality present at the time, day, and part of year that may independently affect delays. Additionally, we believed the region of the country may have an impact, as certain regions have denser populations and others are considered fly over states or have better proximity to other airport across the continental United States.\n",
    "\n",
    "#### Time of Day\n",
    "\n",
    "Based on how many flights to the destination airport remain in the day, or based on airport congestion patterns, airlines may face different pressures which can contribute to delays. We have split the day into three buckets \"late_night\", \"daytime\", and \"evening\" that consist of 10 pm to 6 am, 6 am to 2 pm, and 2 pm to 10 pm, respectively, in order to capture these time-of-day effects.\n",
    "- `late night` [0/1]: Left off because we one-hot encode n-1 features. Flight departure scheduled between 10 pm to 6 am. \n",
    "- `daytime` [0/1]: Flight departure scheduled between 6 am to 2 pm. \n",
    "- `evening` [0/1]: Flight departure scheduled between 2 pm to 10 pm. \n",
    "\n",
    "#### Region of Country\n",
    "\n",
    "The United States consists of several regions, including New England, the Midwest, the West Coast, the South, and the Midatlantic region. These regions are somewhat known for having distinct cultures and character which may contribute to how workers/airports in the region respond to delays. For example, New England sees snow very regularly and will likely respond better than, say, Texas or North Carolina. Additionally, flights scheduled in more far off regions like the Pacific Islands could have a different departure delay than regions more centrally accesible to the rest of the continental US like the Midwest.\n",
    "\n",
    "We have made a list of each state by region (including separate regions for Pacific islands and Atlantic islands) and categorized each state into a region through one-hot encoded binary indicators. We removed `atlantic_islands` to encode n-1 features since the all 0 case is already representing New England based flights.\n",
    "\n",
    "- (categorical: 8 values): `new_england`, `mid_atlantic`, `south`, `midwest`, `southwest`, `west`, `pacific_islands`, `atlantic_islands`.\n",
    "\n",
    "#### Season\n",
    "\n",
    "Although the weather changes by season, airports and airlines change their behaviors as well to deal with this. Aside from weather, the time of year may contribute to delays in other ways. For example, more people tend to take vacations during the summer. Does this change the likelihood of a flight being delayed? Encoding this helps capture any prediction power seasonality might have.\n",
    "\n",
    "In order to capture this, we have used the simplified season definitions for spring, summer, winter, and autumn (3-5, 6-8, 9-11, 12-2 by month number). We one-hot encoded the seasons. We left off `summer` to only one-hot encode n-1 features.\n",
    "\n",
    "- `spring` [0/1]: Flight is scheduled to depart in the Spring.\n",
    "- `summer` [0/1]: Flight is scheduled to depart in the Summer.\n",
    "- `winter` [0/1]: Flight is scheduled to depart in the Winter.\n",
    "- `autumn` [0/1]: Flight is scheduled to depart in the Autumn.\n",
    "\n",
    "#### Weekend/Holiday\n",
    "\n",
    "Similarly to season, flight behavior of consumers changes based on the day of the week (around weekends, to be specific) and proximity to a holiday. We expect that weekends are holidays will have a higher likelihood of delay due to more people tending to travel, so we will use a binary variable to indicate whether the flight took off on a Friday/Saturday/Sunday or within a window around a holiday during which people are likely to travel.\n",
    "\n",
    "We used two \"types\" of holidays (major and minor) and used a 2-day window around the major travel holidays (Christmas and Thanksgiving) and a 1-day window around these minor travel holidays (Fourth of July, Labor Day, Memorial Day, Easter). We also added a range for the most typical spring break dates (March 14 through 31). As mentioned above, we then one-hot encoded each flight for whether it fell on a Friday, Saturday, or Sunday or fell within one of these peri-holiday ranges.\n",
    "\n",
    "- `weekend_or_holiday` [0/1]: Does the flight fall on the weekend or within a window of a holiday observed in the U.S.\n",
    "\n",
    "\n",
    "### Features from Tracking our Plane's Tail Number\n",
    "\n",
    "Associated Notebook: [Feature Engineering 2 Notebook](FeatEng_anand2_Cleaning_Flights_Joined_full.ipynb)\n",
    "\n",
    "Our Exploratory Data Analysis revealed that the tail number of the plane is correlated with the outcome variable we wish to predict: `DEP_DEL15` otherwise known as the indicator for a 15-minute or more delayed departure. The naive approach would be to encode every single tail number into the model, but this can lead to overfitting because of the large number of features added, and it would not be a practical approach for future or seldom seem tail numbers. Instead, we track our flight's tail number prior to our prediction time (2 hours before our scheduled departure) and encode various features that would capture if our plane is caught in a chained delay. A chained delay describes the scenario where a plane was late to depart from its previous airport, so it may be late to arrive at its current airport, and, as a result, be late to depart for its next flight. This propogates delays downward.\n",
    "\n",
    "By examining each flight's tail number, we create the following 5 features:\n",
    "\n",
    "- `prior_dep_delayed` [0/1]: if the most recent flight this airplane took in the last 24 hours, prior to 2 hours before our flight's departure, had a delayed departure. This flight would be the one coming to the current flight's airport. If that flight was late to leave, there is a good chance that it will be late to arrive and cause our flight to be late. If no flight is found in this window, then we assume no delay.\n",
    "- `previous_DEP_DELAY_NEW` [0, infinity]: The departure delay in minutes of the last flight of this airplane, in the last 24 hours prior to 2 hours before our flight's departure. It is a continuous numerical feature that gives the severity of this plane's previous flight departure delay. If there is no delay, then this value is 0 minutes.\n",
    "- `prior_arr_delayed` [0, 1]: if the most recent arrival this airplane had in the last 24 hours, prior to 2 hours before our flight's departure, had a delay. This flight would probably be the one coming to the current flight's airport, but it could be the flight arriving at the airport before our flight's current airport. If that flight was late to arrive at either of these two airports, there is a good chance that our flight will be late because of a delay chain. We would only know this delay if the arrival time happened 2 hours before our departure time, so we respect that in the feature encoding. We check the arrival time of the previous flight and, if it was before our 2 hour end time, we can copy its value for `ARR_DEL15`, otherwise 0 because we assume no arrival delay.\n",
    "- `previous_ARR_DELAY_NEW` [0, infinity]: The arrival delay in minutes of the last flight of this airplane, in the last 24 hours prior to 2 hours before our flight's departure. It is a continuous numerical feature that gives the severity of this plane's previous flight departure delay. If there is no arrival delay, then this value is 0 minutes.\n",
    "- `plane_is_here` [0/1]: check if this plane's last departed flight, 2 to 24 hours prior, arrived already at our current airport. The intuition here is that if our plane is already at our airport, the chance of a delay should likely be lower. Created by checking if the arrival time of this flight is in our window and its destination airport `DEST` matches our origin airport `ORIGIN`. If yes, then 1, otherwise 0.\n",
    "\n",
    "### Features from 24 hour Flight Data Rolling Averages\n",
    "\n",
    "Associated Notebook: [Feature Engineering 2 Notebook](FeatEng_anand2_Cleaning_Flights_Joined_full.ipynb)\n",
    "\n",
    "Our EDA revealed that carrier is correlated with departure delays, but encoding every single carrier can lead to the same issues of overfitting and sparsity that encoding tail number would. As a result, we encode information about a flight's carrier by capturing the proportion of flight departure delays that carrier is experiencing in the last 24 hours prior to our flight prediction. If an airline is more prone to delays than others, and it is experiencing something causing it to have delays in large mass, then this feature will capture that effect. We look from our departure time - 2 hours to our departure time - 26 hours, and get the average delay of this flight's carrier. Missing values will be dropped at the end.\n",
    "- `avg_carrier_delay_24hrs` [0, 1]: Proportion of delayed flights by current flight's carrier in the last 24 hours.\n",
    "\n",
    "Congestion of an airport may influence flight delays at it. One way to characterize congestion at an airport is the number of flights scheduled that day at the airport. Airports always know how many flights they have scheduled for a given day based on ticket booking and coordination with airlines that occurs well in advance of the actual flight. We theorize that an airport that is scheduled to have more flights on a given day might have some relationship with the proportion of delays. We create 2 features, for a flight's origin and destination airport, to give the number of flights that airport had scheduled on this given date.\n",
    "\n",
    "- `flights_sch_Today_ORIGIN` [0, infinity]: number of flights the origin airport scheduled today.\n",
    "- `flights_sch_Today_DEST` [0, infinity]: number of flights the destination airport scheduled today.\n",
    "\n",
    "Our current airport may be experiencing some event that has been leading to a large proportion of delayed flights leaving it. This would certainly be an important predictor of our current flight's delay. We encode the average proportion of delayed flights for our origin airport for the last 24 hours prior to our prediction:\n",
    "- `avg_ori_airport_delay_24hrs` [0, 1]: For each flight, if an origin airport has a lot of delays recently, this feature would reflect the  proportion of delayed flights originating at our origin airport in the past 24 hours before our prediction.\n",
    "\n",
    "### Flight Delay Network PageRank by previous quarter\n",
    "\n",
    "Associated Notebook: [PageRank Notebook](EDA_anand_PageRank_starter.ipynb)\n",
    "\n",
    "On a quarterly basis, we generate a flight graph network where nodes are airports and directed edges are the proportion of delayed flights from one airport to another. PageRank on this network will give each airport a probability of being the originator or recepient of a delayed flight, based on the direction of the directed edges. If the current flight's origin airport had a high PageRank for originating departure delayed flights last quarter, then our current airport seems likely to send out delayed flights. If our current flight's destination airport had a high PageRank for receiving departure delayed flights last quarter, then our flight has a higher likelihood of being delayed on its way to this destination. \n",
    "- `depDelayPageRank` (0, 1): This gives the PageRank of the origin airport on the network where edges into a node are proportion of delayed flights flying from the origin. More important nodes in this network will have a higher chance of having delayed flights fly from them. This captures the \"chance that our flight is delayed, based on our origin airport having a greater proportion of departing flights delayed\".\n",
    "- `arrDelayPageRank` (0,1): This gives the PageRank of the destination airport on the network where edges into a node are proportion of delayed flights flying to the destination. More important nodes in this network will have a higher chance of having delayed flights fly into them. This captures the \"chance that our flight is delayed, based on our destination airport having a greater proportion of arriving flights that were delayed\".\n",
    "\n",
    "Having both of these features when both PageRank values are high, should encode that this flight path is particularly likely to be delayed because it starts from an airport that sends out many delayed flights and is ending at an airport that receives many delayed flights. \n",
    "\n",
    "### Kept Features from Flights\n",
    "\n",
    "We kept the following features post join from our flight data, in case it provided additional prediction info for `DEP_DEL15`:\n",
    "- `DISTANCE` [0, infinity]: Distance between origin and destination airports.\n",
    "- `ORI_elevation` [-infinity, infinity]: Elevation of the origin airport.\n",
    "- `DEST_elevation` [-infinity, infinity]: Elevation of the destination airport.\n",
    "\n",
    "\n",
    "### Features added Post-Baseline\n",
    "\n",
    "Associated Notebook: [Feature Engineering 3 Notebook](FeatEng_anand3_Cleaning_Flights_Joined_full.ipynb)\n",
    "\n",
    "After our logistic regression baseline, we attempted to improve our models moving forward by creating more features from the flight data.\n",
    "\n",
    "#### Hourly Delay Rolling Average\n",
    "\n",
    "- `avg_hourly_delay_24hours` [0, 1]: In the last 24 hours, checking the average delay per hour can indicate an issue with flights just ahead of our current flight. Intuitively, recent flights should inform current flights, and perhaps those flights in the last 24 hours might influence our current flight's delay.\n",
    "  - Missing `avg_hourly_delay_24hr`: 8232 (0.026% of records)\n",
    "    - This likely is due to the flights on the first day of the dataset. Populate with the average delay from 2014, Q4.\n",
    "\n",
    "#### Origin-Destination Route's Delay Rolling Averages\n",
    "\n",
    "Associated Notebook: [Feature Engineering 3 Notebook](FeatEng_anand3_Cleaning_Flights_Joined_full.ipynb)\n",
    "\n",
    "In the last 24 hours, we encoded the average delay proportion, average delay length, and the number of flights taken along each route (origin to destination) in the dataset.\n",
    "\n",
    "- `avg_OD_dep_del15_24hr` [0,1]: In the last 24 hours, what proportion of flights made from this origin to destination were delayed?\n",
    "  - `avg_OD_dep_del15_24hr`: 431838 (1.39% of records)\n",
    "    - Likely due to this route not being flown before in the last 24 hours. Populate with the average departure delay of this origin airport to other destination airports in the last 24 hours.\n",
    "- `avg_OD_delay_min_24hr` [0, infinity]: In the last 24 hours, what was the average delay in minutes of flights made from this origin to destination? Gives the severity of the previous delay.\n",
    "  - `avg_OD_delay_min_24hr`: 431838 (1.39% of records)\n",
    "    - Likely due to this route not being flown before in the last 24 hours. Populate with the average departure delay in min (DEP_DELAY_NEW) of this origin airport to other destination airports in the last 24 hours.\n",
    "- `avg_OD_num_flights_24hr` [0, infinity]: In the last 24 hours, how many times has this flight been flown? A flight flying often or seldomly may inform the context of other features. For example, we would expect that high average delay for this route matters more if there were many flights flown on this route.\n",
    "\n",
    "#### Delay PageRanks as Ordinal Features\n",
    "\n",
    "Associated Notebook: [Feature Engineering 3 Notebook](FeatEng_anand3_Cleaning_Flights_Joined_full.ipynb)\n",
    "\n",
    "Perhaps the PageRank features, being quarterly, have their numeric values mean something different at the beginning and end of our training data period. If we encoded the PageRank as rank, where 1 means the most delay-likely airport, then the feature across all quarters would share that relationship and have a more consistent cutoff for tree models. We encoded the PageRank features as ranked ordinals in the quarters that they were generated for.\n",
    "\n",
    "- `depDelayPageRank_ordinal` [1, infinity]: depDelayPageRank's ordinal value for the quarter. Lower number means greater delay airport.\n",
    "- `arrDelayPageRank_ordinal` [1, infinity]: arrDelayPageRank's ordinal value for the quarter. Lower number means greater delay airport.\n",
    "\n",
    "\n",
    "#### Plane on its way? Time between flights.\n",
    "\n",
    "Associated Notebook: [Feature Engineering 3 Notebook](FeatEng_anand3_Cleaning_Flights_Joined_full.ipynb)\n",
    "\n",
    "The status of the plane flying into our current airport, tracked via tail number, is captured by 3 scenarios identified below:\n",
    "\n",
    "#### Figure 21\n",
    "![3 scenarios for planes coming in.](time_inb_flights_diagram.PNG)\n",
    "\n",
    "These scenarios describe the previous flights taken by our plane prior to prediction time, and if that plane is coming to our current airport. Encoding which of these scenarios exists for the current flight requires `is_plane_here` and a variable for if the plane `departed_for_current_aiport`. The time in between the last two flights before prediction depends on which of these scenarios is happening, so the calculation is adjusted.\n",
    "\n",
    "- `departed_for_current_aiport` [0/1]: Is the most recent departing flight we can see before prediction heading for our current airport? Essentially checks if the plane left for our current airport already before the prediction time.\n",
    "- `time_inb_flight_min` [0, infinity]: Perhaps the time in between flights, i.e. the time between a flight's arrival and next departure, has a relationship with departure delay. A flight with more idle time could be better prepared for flying on time, or it could propogate a delay in the future. We can encode the time in between flights for each flight, based on its last observable time between flights. The 3 possible scenarios, respecting our 2 hour prior prediction, are outlined in the diagram. When we cannot calculate this quantity, we impute with 0.\n",
    "  - 95534 missing values due to off nominal flights that cannot find flights within the window by the tail number. Imputed by: setting to 0 minutes.\n",
    "\n",
    "#### Time Between Flights Rolling Averages\n",
    "\n",
    "Associated Notebook: [Feature Engineering 3 Notebook](FeatEng_anand3_Cleaning_Flights_Joined_full.ipynb)\n",
    "\n",
    "We tried features for the time in between flights based on the carrier, origin, and destination as rolling 24 hour averages. \n",
    "- `avg_time_inb_flights_carrier_24hr` [0, infinity]: Looking at the carrier, what is the average time in between their flights in the last 24 hours.\n",
    "  - Missing `avg_time_inb_flights_carrier_24hr`: 608 (0.002% of records)\n",
    "    - Likely due to flights on the first day of the dataset not having flights in their look back window of 24 hours. Impute with 0 min average time between flights because this was the imputation for unfounded data when we cannot calculate `time_inb_flight_min`\n",
    "- `avg_time_inb_flights_origin_24hr`  [0, infinity]: Looking at the origin airport, what is the average time in between flights starting from here in the last 24 hours.\n",
    "  - Missing `avg_time_inb_flights_origin_24hr` 11197 (0.03% of records)\n",
    "    - Could be due to new origin airports, no flights in the first day's look back window. Impute with 0.\n",
    "- `avg_time_inb_flights_dest_24hr`  [0, infinity]: Looking at the destination airport, what is the average time in between flights going there in the last 24 hours.\n",
    "  - Missing `avg_time_inb_flights_dest_24hr` 10827 (0.03% of records)\n",
    "    - Likely due to flights on the first day of the dataset not having flights in their look back window of 24 hours. Impute with 0 min average time between flights because this was the imputation for unfounded data when we cannot calculate `time_inb_flight_min`\n",
    "  \n",
    "#### Origin Airport Rolling Averages\n",
    "\n",
    "Associated Notebook: [Feature Engineering 3 Notebook](FeatEng_anand3_Cleaning_Flights_Joined_full.ipynb)\n",
    "\n",
    "The 24 hour rolling average of departure delay length at the origin airport could be long as a result of persistent bad weather or location specific delays, so we encoded this for each flight. \n",
    "- `avg_ori_DEP_DELAY_NEW_24hr` [0, infinity]: Looks at the average departure delay in minutes 24 hours prior to prediction of the flights taken from our origin airport. If flights from our airport have been delayed for long, this could signify that our flight will be delayed.\n",
    "  - Missing `avg_ori_DEP_DELAY_NEW_24hr` 11197 (0.03% of records)\n",
    "      - Could be due to new origin airports, no flights in the training data first day's look back window (1/1/2015). Impute with the average delay in minutes from the day before (12/31/2014).\n",
    "      \n",
    "#### New Quarterly Features\n",
    "\n",
    "Associated Notebook: [Feature Engineering 3 Notebook](FeatEng_anand3_Cleaning_Flights_Joined_full.ipynb)\n",
    "\n",
    "Some features are populated using a look up table of their values from the previous quarter, to prevent data leakage and allow our models to predict on unseen data since previous quarter data is always available. For these, we need to enumerate quarters in our flights data and join with the quarterly values on the previous quarter.\n",
    "\n",
    "##### Carriers\n",
    "\n",
    "Associated Notebook: [Feature Engineering 3 Notebook](FeatEng_anand3_Cleaning_Flights_Joined_full.ipynb)\n",
    "\n",
    "We encoded categorical data on the carrier as ordinal data using Brieman’s Method on each carrier's average departure delay from last quarter. We produced several features from this metric to capture the proportion of delay and severity of the delay in minutes. We also produced a binary indicator for each carrier to signal if the average delay for this carrier exceeded 15 minutes.\n",
    "\n",
    "- `airline_carrier_del` [0, 1]: The average carrier delay for this flight's carrier from last quarter.\n",
    "- `airline_carrier_del_min` [0, infinity]: The average delay in minutes of this carrier from last quarter.\n",
    "- `avg_carrier_delay_over15_lastQ` [0/1]: Binary indicator saying if this carrier's average delay time is over 15 minutes; our threshold for a delay.\n",
    "- `airline_carrier_del_ordinal` [1, infinity]: The ordinal rank of this carrier based on delay rate last quarter. 1 means the most delayed carrier.\n",
    "- `airline_carrier_del_min_ordinal` [1, infinity]: The ordinal rank of this carrier based on delay time last quarter. 1 means the longest delay-having carrier.\n",
    "\n",
    "Missing values, usually due to this carrier not existing in the previous quarter, are imputed with means from the previous quarter across all the carriers. The `avg_carrier_delay_over15_lastQ` missing values are then recomputed.\n",
    "\n",
    "### Origin Airports\n",
    "\n",
    "Associated Notebook: [Feature Engineering 3 Notebook](FeatEng_anand3_Cleaning_Flights_Joined_full.ipynb)\n",
    "\n",
    "We can encode Origin Airports, previously categorical string names, as ordinal values by using Brieman's method for ordering them using the past quarter data airport mean values for `DEP_DEL15`. We have to use the past quarter’s data because using the current quarter averages is data leakage from later averages, and this approach would not translate to unseen data where quarterly averages are not available. The past quarter could be close enough in time to relevantly capture the current quarter flight behavior at this airport.\n",
    "\n",
    "- `origin_avg_DEP_DEL15` [0,1]: proportion of departing flights delayed at the origin airport last quarter.\n",
    "- `origin_avg_DEP_DELAY_NEW` [0,infinity]: average delay in min of flights at the origin airport last quarter.\n",
    "- `avg_origin_delay_over15_lastQ` [0/1]: if `origin_avg_DEP_DELAY_NEW` is greater than 15 min, signaling a delayed departure.\n",
    "- `origin_del_ordinal` [1, infinity]: ordinal rank of origin airport by proportion of delays.\n",
    "- `origin_del_min_ordinal` [1, infinity]: ordinal rank of origin airport by length of delays.\n",
    "\n",
    "\n",
    "Missing values, usually due to this airport not existing in the previous quarter, are imputed with means from the previous quarter across all the airports. The `avg_origin_delay_over15_lastQ` missing values are then recomputed.\n",
    "\n",
    "\n",
    "### Route Origin-Destination \n",
    "\n",
    "Associated Notebook: [Feature Engineering 3 Notebook](FeatEng_anand3_Cleaning_Flights_Joined_full.ipynb)\n",
    "\n",
    "We encoded the categorical routes defined as Origin-Destination pairs as ordinal using their values of `DEP_DEL15` and their delay in minutes `DEP_DEL_NEW` from the previous quarter. There is a possibility that the routes themselves can be correlated with delays due to paths taken or infrastructure supporting those routes.\n",
    "\n",
    "- `OD_avg_DEP_DEL15` [0,1]: proportion of departing flights delayed on this route last quarter.\n",
    "- `OD_avg_DEP_DELAY_NEW` [0,infinity]: average delay in min of flights on this route last quarter.\n",
    "- `avg_OD_delay_over15_lastQ` [0/1]: if `OD_avg_DEP_DELAY_NEW` is greater than 15 min, signaling a delayed departure.\n",
    "- `OD_del_ordinal` [1, infinity]: ordinal rank of route by proportion of departure delays.\n",
    "- `OD_del_min_ordinal` [1, infinity]: ordinal rank of route by length of departure delays.\n",
    "\n",
    "Missing values, usually due to this route not existing in the previous quarter, are imputed with means from the previous quarter across all routes. The `avg_OD_delay_over15_lastQ` missing values are then recomputed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "84c7117f-f581-4197-9671-f06a0bab7ac1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Data Cleaning\n",
    "\n",
    "### Cancelled Flights, Missing `DEP_DEL15`, and Duplicates\n",
    "\n",
    "Prior to the join, we made the decision to drop flights that resulted in cancellations or had null values for our outcome variable `DEP_DEL15` ([command 5 in this notebook](pipeline_full_data_join.ipynb)). Cancelled flights are distinct from delayed flights for our airline stakeholder because the methods for planning around a delayed flight are fundamentally different than how airlines will handle a cancelled flight. For example, an airline would not reschedule staff promptly to crew a cancelled flight as they would a delayed flight. Therefore, our predictions should not conflate cancellations with delays. \n",
    "\n",
    "Additionally, we found true duplicate flight records (matches in every column), which we removed prior to joining.\n",
    "\n",
    "\n",
    "### Flights Data (Features)\n",
    "\n",
    "[Section: Cleaning Flight Feature Columns of Nulls](FeatEng_anand2_Cleaning_Flights_Joined_full.ipynb)\n",
    "\n",
    "The following features in flight data had missing values after creation:\n",
    "- `DEST_elevation`: 159142 (0.5% of records)\n",
    "  - This likely is due to the subset of flights flying from US airports to airports who did not have their elevations included. Since this is a small portion of the data, we can fill the missing values with the average `DEST_elevation`. There is no risk of data leakage when doing this because the elevation of an airport is independent of other flight records and should not change over time. The strategy for unseen data could simply be to impute this same average for past flights' destination elevations. \n",
    "- `avg_carrier_delay_24hrs`: 608 (0.002% of records)\n",
    "  - Likely due to first flights on the first day of the dataset (1-1-2015) not having any flights in their look back window of 24 hours. This is such a small portion of data. Easy option would be setting these average carrier delays to 0. Imputing with any averages risks data leakage if the calculation of the average for all carriers or just this one includes future values. Either imputing as 0 or dropping the rows seems better than average. We imputed the mode of the 2014 Q4 data. Our training data begins in 2015.\n",
    "- `avg_ori_airport_delay_24hrs`: 11197 (0.03% of records)\n",
    "  - Partially due to first flights on the first day of the dataset (1-1-2015) not having any flights in their look back window of 24 hours. Also might be due to flights that do not have a flight from this airport in the last 24 hours. These could be smaller airports, which make sense to impute 0 as. Best to handle them the same way as `avg_carrier_delay_24hrs`. We imputed the mode of the 2014 Q4 data. Our training data begins in 2015.\n",
    "\n",
    "\n",
    "### Weather Data\n",
    "\n",
    "We had a mix of numerical and categorical variables from the weather dataset. The first step was to unnest the columns. For numeric features, we would save one of the nested values as an integer and use any nested values related to the observation quality to \n",
    "convert erroneous observations into `Null` values. For example, we unnested the weather data variable `OC1` into two values, the speed rate and the quality code of the speed rate. We saved the speed rates into our new table and converted them into integers, except for ones that corresponded to quality code 3 and 7, which the weather dictionary defined as codes for erroneous data, and converted those speed rates to `Null`. We had considered simply filtering out the rows with erroneous observations, however, when we did we were left with very few rows in our joined dataset. For some numerical variables like `WA1` that observed ice thickness, missing or erroneous values were saved as 0 in the data table with the logic being that if there was no observation reported that might mean that there was no ice that day and thus the station did not report anything.\n",
    "\n",
    "[Weather data cleaning](Weather_Imputation.ipynb)\n",
    "\n",
    "#### Figure 22\n",
    "\n",
    "![Weather to indicators](weather_data_indicators.PNG)\n",
    "\n",
    "For categorical variables, we decided to convert them into binary indicator variables since some of them, like AW1, had 100 possible values. For the variables `AA1` and `MW1`, automated and manual present weather observation, we created \n",
    "indicator variables combining values they had that observed similar weather patterns. For example, if `AW1 == 99 or MW1 == 19` then we would save a 1 for that value in a derived variable that indicated if a tornado was [observed](Unnesting_weather_in_joined_data.ipynb). \n",
    "The chart above shows how we split up the possible values in a varibale, in this case `AW1` into multiple indicator variables.\n",
    "\n",
    "#### Weather Imputation\n",
    "[Imputation Notebook](Weather_Imputation.ipynb)\n",
    "\n",
    "Upon unnesting the weather values, we discovered many reports that contained null values. To remedy this, we tried to impute the most relevant weather value by first looking at that station's average value for that variable during that month. Though this filled some values, many nulls remained. Next, we considered each of that stations reports for the same month across the years of the data and filled using this average. We debated whether averaging across times (some of which may be in the future, relative to the point of reference), but ultimately we came down on the side that it is acceptable (weather is not an outcome variable and the average itself should not contain enough information to present leakage issues).\n",
    "\n",
    "After producing this table, nulls remained, so we next averaged all of that station's reports for every month and year. To our surprise, even this step contained nulls (some stations, it appears, never report certain values). So for one final step, to solve the last few nulls, we averaged across all stations for all months and years.\n",
    "\n",
    "We used these aggregate tables to impute \"down the chain\" toward the most granular data, filling values with an average only where null values existed for that observation. From all stations, to all months, to that month across years, to that station-month, until finally we had no remaining nulls.\n",
    "\n",
    "### PageRank Features\n",
    "\n",
    "[PageRank notebook](EDA_anand_PageRank_starter.ipynb)\n",
    "\n",
    "After generating our 2 PageRank delay features, we had 20,947 missing `depDelayPageRank` & `arrDelayPageRank` values from 31,080,934 flights (~0.07%). This is because the previous quarter of the flight did not have this current flight's origin airport in any flights that quarter, so we did not find a match. This can happen when the airports in the network change next quarter. We impute these nulls with the previous quarter's mean `depDelayPageRank` and `arrDelayPageRank` values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2ac1e6a5-7a23-42a0-8d04-3607b7c32f43",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 5. Algorithm Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b4f055f6-96de-4996-92d2-27a8ac004c09",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 5.1: Cross Validation\n",
    "\n",
    "Refrence: [Sample Code of 5year Cross Validation Split / Custom Cross Validation Split](Second_model_RF_custom_CV_and_voting_draft_rumi.ipynb)\n",
    "\n",
    "\n",
    "Since we deal with time series data in this project, random split function such as [`CrossValidator`](https://spark.apache.org/docs/latest/ml-tuning.html) is not appropriate due to the potential data leakage. To avoid data leakage, [Block Time Series Split](https://hub.packtpub.com/cross-validation-strategies-for-time-series-forecasting-tutorial/)(rolling cross validation) was applied. In short, it is a method to avoid having holdout set before the period of train set, and data needed to be manually splitted to achieve it. Another optioin was to expand the period in each cross validation split group, while always having holdout set periodically later than train set. It enables to contain larger size for train set, however, older records are more often used for train set. Because of the architectural limitations, model should be more likely to be optimized with older dataset. On the other hand, rolling window enables to avoid the ocurrance of the \"overfitting\" due to the split, we chose blocking time series as a basic direction of the dataset. \n",
    "\n",
    "#### Figure 23 Cross Validation Split for Five Years Data \n",
    "\n",
    "![Original CV](Screen_Shot_2022_04_10_at_5_15_09.png)\n",
    "\n",
    "Figure 23 shows how we have done split for 5-year dataset. We had six months. for train set and two months for holdout set in each split group, and had nine groups as total. There were three basic concepts behind the decision. Firstly, we assumed keeping 1-year period for final test is appropriate to remove bias of potential seasonality effect. As we saw there were slight correlation between features related to snow fall and the target variable, this model is likely to have dependence in seasonality. Secondly, also because of the concern against seasonality, we also avoided having repetition of same monthly period across all group.(Ex. splitting group to every January-June and July-December was avoided). Thirdly, balance between the length of each training set, and having enough large number of groups was taken into account. \n",
    "\n",
    "Looking ahead of the application in practice, the code is designed to work with new data. Assuming modeling is done in the latest five-year dataset, code finds the latest record of the dataset first, and then go back to five years ago to find the split point of the group (Figure 24). In the later phase of the project, design of the split was updated, as is shown in Figure 25. \n",
    "\n",
    "#### Figure 24 Architectural Design of Cross Validation Split \n",
    "![Architecture data works with new data](Screen_Shot_2022_04_10_at_5_15_42.png)\n",
    "\n",
    "Update was made due to the two potential challenges - One is the possibility of loosing the capability of reflecting seasonality in the prediction. Having six months for training period , followed by shifting the staring month/end month in each split group, overal model would likely to apply seasonality for prediction. The other reason is the concern of big O(n) by having nine groups. There are generally trade off between having larger amount of records in each group and having more groups with smaller number of records, but we found that in Train time can proportionally Hyperparameter tuning time required to train nine groups. It became challenging in hyperparameter tuning, because the training time proprotionally increased by having more split groups. \n",
    "\n",
    "In order to build a better model, the architecture of cross validation split were redigned. Firstly, the period was expanded to a year in both train data and test data in order tobuild the model that uses the information of seasonality effectively. Secondly, latest records were more likely to used, assuming that newer records from present predicts better. We added one group that included first half of year in 2018, which was not able to include through 1 year-1 year split, and assigned half a year for holdout set. We also added a group that started at July 2016 to add more impact of the newer records to final prediction. We pivoted to the following CV fold for models post-baseline (namely XGBoost).\n",
    "\n",
    "### Figure 25 Custom Cross Validation Split  \n",
    "![Custom CV](Screen_Shot_2022_04_10_at_5_14_41.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "658fb496-249c-46c3-b2ab-5b68879de717",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 5.2: Logistic Regression\n",
    "\n",
    "For our baseline model we decided to use logistic regression, an algorithm that determines the probabilty of an event occuring or not occuring.\n",
    "We decided to use a logistic regression as our baseline model because it is fairly quick to implement and train, easier to interpret than other models and works well with binary outcome variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "04262be7-42e0-439a-8da3-1c04c4302957",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 5.2.1: Baseline (All Features)\n",
    "\n",
    "- [Logistic Regression Baseline-2 (All Features)](Feature_selection_LR_airlinejoin_baseline-2.ipynb)\n",
    "- [Logistic Regression Baseline-3 (8 Features)](Feature_selection_LR_airlinejoin_baseline-3.ipynb)\n",
    "\n",
    "We had 49 features at the time of our baseline. Our baseline of logistic regression was run 3 times. \n",
    "\n",
    "1) Using all 49 features with a greedy search hyperparmeter tuning.\n",
    "2) Using all 49 features with a greedy/grid search hybrid hyperparameter tuning.\n",
    "3) Using 8 features, selected by Random Forests, using greedy search hyperparmeter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ab13c968-6f1e-443a-828b-d687c8844043",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Greedy Search Hyperparameter Tuning\n",
    "\n",
    "Associated Notebook: [Logistic Regression Baseline-2 (All Features)](Feature_selection_LR_airlinejoin_baseline-2.ipynb)\n",
    "\n",
    "Using all 49 features created at the time of our baseline, we produced a logistic regression model that used greedy search to tune the following hyperparameters.\n",
    "\n",
    "| Hyperparameter    | Description                                                                                                               | Search Space                                                     |\n",
    "|-------------------|---------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------|\n",
    "| Rebalancing Ratio | The ratio of desired negative to positive samples in a given training set. Used with undersampling the majority class.    | [0.7, 1, 1.3, 1.6, 1.9, 2.1, 2.4, 2.7, 3.0]                      |\n",
    "| MaxIter           | Number of max iterations to do Logistic Regression gradient descent over.                                                 | [10, 15, 20, 25, 30, 35, 40, 45, 50]                             |\n",
    "| regParam          | The L2 regularization parameter, or the regularization parameter for L1 & L2 when using a non-zero elastic net parameter. | [0, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100, 1000] |\n",
    "| elasticNetParam   | The elastic net tuning parameter that balances L2 and L1 regularization. 0 means L2, 1 means L1.                          | [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]              |\n",
    "\n",
    "##### Undersampling Non-Delayed Flights Majority Class\n",
    "\n",
    "In the full training dataset we have the following breakdown of negatives (no delays) to positives (delays). As we can see, the dataset of flights is heavily imbalanced. Only 22% of flights are delayed. This imbalance may prevent our model from learning how to predict delays because it will not see enough examples of delayed flights. We chose to rebalance our flight data through undersampling non-delayed flights.\n",
    "\n",
    "| DEP_DEL15 | count    |\n",
    "|-----------|----------|\n",
    "| 0         | 19531434 |\n",
    "| 1         | 4297626  |\n",
    "\n",
    "We created a procedure to randomly sample from the negative class to achieve a desired ratio rebalancing of negatives to positives. This is found in our baseline notebooks initially. Here is an approximation of 1.6 rebalancing ratio (which we found to be the optimal tuned parameter):\n",
    "\n",
    "| DEP_DEL15 | count    |\n",
    "|-----------|----------|\n",
    "| 0         | 6877315  |\n",
    "| 1         | 4297626  |\n",
    "\n",
    "Rebalancing the negatives to positives of `DEP_DEL15` in the training set improved the best f-1 score for Logistic Regression to 0.486 from 0.42 when not using any rebalancing (See [notebook for original with no rebalancing in the “Run on 2019 test set” section](Feature_selection_RF_airlinejoin_baseline.ipynb)). Therefore, we chose to include rebalancing in the baseline and subsequent modeling.\n",
    "\n",
    "\n",
    "##### Optimal Hyperparameters\n",
    "\n",
    "Using greedy search, we looked for the best hyperparameters in the following order for the baseline: rebalancing ratio → maxIter → regParam → elasticNetParam.\n",
    "\n",
    "Using greedy search for hyperparameter tuning, we traded for speed and the ability to check more individual hyperparameter values for the trade off of possibly biasing our search space depending on the order of how we looked for hyperparameters. Using a full grid search would have been ideal, but its trade off involves an enormous computation time that does not scale well with the search space and perform quickly enough for this large amount of data. This is especially true for our multiple cross validation splits each requiring a set of hyperparameters training over them every time. The middle ground between these two approaches is random search for hyperparameters, which involves randomly choosing hyperparameter values to explore the search space. This avoids biasing from your starting location. We applied this in our advanced model post-baseline. \n",
    "\n",
    "We chose the hyperparameter that produced the highest average f1 score across all CV folds. The optimal hyperparameter for this model, based on this greedy search across our CV split, returned the following performance on the 2019 test set.\n",
    "\n",
    "\n",
    "\n",
    "| Hyperparameter Tuning Style for CV | Features | Rebalancing Ratio | maxIter | regParam | elasticNetParam | Recall | Precision | F1_score |\n",
    "|------------------------------------|----------|-------------------|---------|----------|-----------------|--------|-----------|----------|\n",
    "| Greedy Grid Search                 | 49 (all during baseline) | 1.6               | 10      | 0        | 0               | 0.466  | 0.507     | 0.486    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4f386734-4697-4b0d-9768-64a8babb3d7d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Greedy Search & Grid Search Hyperparameter Tuning\n",
    "\n",
    "Associated Notebook: [Logistic Regression Baseline-2 (All Features)](Feature_selection_LR_airlinejoin_baseline-2.ipynb)\n",
    "\n",
    "\n",
    "A full greedy search can miss the optimal hyperparameter values due to its sensitivity to the starting location and order of hyperparameters searched. We explored a hybrid greedy search, grid search approach to hyperparameter tuning. \n",
    "\n",
    "The following hyperparameters were still tuned in a greedy fashion: rebalancing ratio → maxIter. Then we did a grid search over combinations of regParam and elasticNetParam. The motivation for this was because our previous model found the best regParam to be 0, so any subsequent elasticNetParam could not be tested. We would like to evaluate more than just L2 regularization. By doing a grid search over these two, we exhaustively searched the space of elastic net regularization for the model above. This came at the cost of additional cross validation training time. This table summarizes the tuning strategy.\n",
    "\n",
    "\n",
    "\n",
    "| Hyperparameter    | Description                                                                                                               | Search Space                                |\n",
    "|-------------------|---------------------------------------------------------------------------------------------------------------------------|---------------------------------------------|\n",
    "| Rebalancing Ratio | The ratio of desired negative to positive samples in a given training set. Used with undersampling the majority class.    | [0.7, 1, 1.3, 1.6, 1.9, 2.1, 2.4, 2.7, 3.0] |\n",
    "| MaxIter           | Number of max iterations to do Logistic Regression gradient descent over.                                                 | [10, 15, 20, 25, 30, 35, 40, 45, 50]        |\n",
    "| regParam          | The L2 regularization parameter, or the regularization parameter for L1 & L2 when using a non-zero elastic net parameter. | [0.001, 0.005, 0.01, 0.05, 0.1]             |\n",
    "| elasticNetParam   | The elastic net tuning parameter that balances L2 and L1 regularization. 0 means L2, 1 means L1.                          | [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]    |\n",
    "\n",
    "\n",
    "Despite doing a more exhaustive search, the parameters produced produce a higher f1 score than our previous model above. Here are the results from this model.\n",
    "\n",
    "\n",
    "| Hyperparameter Tuning Style for CV                 | Features | Rebalancing Ratio | maxIter | regParam | elasticNetParam | Recall | Precision | F1_score |\n",
    "|----------------------------------------------------|----------|-------------------|---------|----------|-----------------|--------|-----------|----------|\n",
    "| Greedy + grid search on regParam & elasticNetParam | 49 (all) | 1.6               | 10      | 0.001    | 0.5             | 0.509  | 0.451     | 0.478    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6eb0a368-a25a-4b2f-98e7-6f867a227dd0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 5.2.2: Baseline (Feature Selection via Random Forests)\n",
    "\n",
    "\n",
    "At the time of our baseline model, we did not create the PageRank delay features and other features in the **Post Baseline** section of our **Derived Flight Features** section so these were not included in the first random forest feature selection search. Our search yielded 8 features that accounted for 97% of the Feature Importance. We used these 8 features in a Logistic Regression model with hyperparameters tuned using greedy search to produce a baseline model. \n",
    "\n",
    "These 8 features were:\n",
    "```\n",
    "['prior_dep_delayed', 'previous_DEP_DELAY_NEW', 'plane_is_here', 'evening', 'previous_ARR_DELAY_NEW', 'avg_carrier_delay_24hrs', 'prior_arr_delayed', 'avg_ori_airport_delay_24hrs']\n",
    "```\n",
    "\n",
    "#### Figure 26\n",
    "![Random Forest Feature Selection for Baseline](feat_select_RF_1.PNG)\n",
    "\n",
    "\n",
    "The associated notebook for feature selection is found [here](Second_model_RF_custom_CV_and_voting_draft_rumi.ipynb).\n",
    "\n",
    "\n",
    "---\n",
    "#### Baseline (Top 8 Features with 97% Importance)\n",
    "\n",
    "[Logistic Regression Baseline-3 (8 Features)](Feature_selection_LR_airlinejoin_baseline-3.ipynb)\n",
    "\n",
    "\n",
    "Using these 8 features only, we re-tuned the hyper parameters of our baseline logistic regression model using greedy search, since it yielded better results than our greedy search/grid search hybrid above. \n",
    "\n",
    "The hyper parameters we searched over are as follows:\n",
    "\n",
    "| Hyperparameter    | Description                                                                                                               | Search Space                                                     |\n",
    "|-------------------|---------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------|\n",
    "| Rebalancing Ratio | The ratio of desired negative to positive samples in a given training set. Used with undersampling the majority class.    | [0.7, 1, 1.3, 1.6, 1.9, 2.1, 2.4, 2.7, 3.0]                      |\n",
    "| MaxIter           | Number of max iterations to do Logistic Regression gradient descent over.                                                 | [10, 15, 20, 25, 30, 35, 40, 45, 50]                             |\n",
    "| regParam          | The L2 regularization parameter, or the regularization parameter for L1 & L2 when using a non-zero elastic net parameter. | [0, 0.001, 0.0025, 0.005, 0.01, 0.025, 0.05, 0.1, 0.5, 1, 5, 10] |\n",
    "| elasticNetParam   | The elastic net tuning parameter that balances L2 and L1 regularization. 0 means L2, 1 means L1.                          | [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]              |\n",
    "\n",
    "\n",
    "The best model was determined by the best hyperparameters, chosen by the highest average f1 score across CV folds, did not outperform the initial baseline on the 2019 test set. It still found the same Rebalancing ratio, maxIter, and 0 elasticNetParam to be ideal. Though it included some L2 regularization.\n",
    "\n",
    "| Hyperparameter Tuning Style for CV | Features                    | Rebalancing Ratio | maxIter | regParam | elasticNetParam | Recall | Precision | F1_score |\n",
    "|------------------------------------|-----------------------------|-------------------|---------|----------|-----------------|--------|-----------|----------|\n",
    "| Greedy Search                      | 8 (97% importance  from RF) | 1.6               | 10      | 0.025    | 0               | 0.446  | 0.512     | 0.477    |\n",
    "\n",
    "\n",
    "However, this result shows promise for random forest feature selection. The difference between 0.486 and 0.477 f1 score is not very large, and it was achieved through a downselection of 49 → only 8 features. Having a model described by fewer features could prevent overfitting in test data outside of 2019, even though it did not outperform full features on this model. We use random forest feature selection again on our full, final feature list.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b1715352-b037-42ca-9047-02b046991d40",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Summary table of Baseline Models: Logistic Regression\n",
    "\n",
    "| Hyperparameter Tuning Style for CV              | Features                    | Rebalancing Ratio | maxIter | regParam | elasticNetParam | Recall | Precision | F1_score |\n",
    "|-------------------------------------------------|-----------------------------|-------------------|---------|----------|-----------------|--------|-----------|----------|\n",
    "| Greedy Grid Search                              | 49 (all)                    | 1.6               | 10      | 0        | 0               | 0.466  | 0.507     | 0.486    |\n",
    "| Greedy + cross tuning regParam, elasticNetParam | 49 (all)                    | 1.6               | 10      | 0.001    | 0.5             | 0.509  | 0.451     | 0.478    |\n",
    "| Greedy Search                                   | 8 (97% importance  from RF) | 1.6               | 10      | 0.025    | 0               | 0.446  | 0.512     | 0.477    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "14845808-bded-48f1-81ec-2811fd430e1c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 5.2.3: New Features Added to Improve Models Going Forward\n",
    "\n",
    "After our baseline, we believed that relationships between our features and delays may be non-linear since the f1 scores were not improving for logistic regression. We ran initial tests of Random Forests models to capture that non-linearity to improve our f1 score. The initial investigation of random forests, with no tuning of hyperparameters and rebalancing ratio of 1.6, yielded a 9 fold cross validation of 0.473 (See notebook [here, section Run Cross Validation](Second_model_RF_hyperparametertuning_anand.ipynb). For reference, our best logistic regression baseline had a 9 fold cross validation of 0.473 as well. \n",
    "\n",
    "| maxDepth | maxBins | minInstancesPerNode | numTrees | desired_neg_to_pos_ratio | Accuracy | Recall | Precision | f1_score |\n",
    "|----------|---------|---------------------|----------|--------------------------|----------|--------|-----------|----------|\n",
    "| 6        | 64      | 100                 | 20       | 1.6                      | 0.8305   | 0.4207 | 0.5433    | 0.4730   |\n",
    "\n",
    "We anticipated an increase in f-1 score out of the box for a random forest model if our features were non-linear. Since we did not observe that increase here, we decided that another effort into feature engineering was warranted. Perhaps the 49 features we had for our baseline were not able to describe very much of the explainable error in the problem of predicting flight delays. Adding more flight features, that could potentially predict flight delays, was a parallel effort to tree model development. \n",
    "\n",
    "The features that we created were detailed in the **Derived Flight Features** section and listed again below:\n",
    "\n",
    "```\n",
    "['avg_hourly_delay_24hr', 'avg_OD_dep_del15_24hr', 'avg_OD_delay_min_24hr', 'avg_OD_num_flights_24hr', \n",
    "'depDelayPageRank_ordinal', 'arrDelayPageRank_ordinal', 'departed_for_current_aiport', 'time_inb_flight_min', \n",
    "'avg_time_inb_flights_carrier_24hr', 'avg_time_inb_flights_origin_24hr', 'avg_time_inb_flights_dest_24hr', \n",
    "'avg_ori_DEP_DELAY_NEW_24hr', 'airline_carrier_del', 'airline_carrier_del_min', 'avg_carrier_delay_over15_lastQ', \n",
    "'airline_carrier_del_ordinal', 'airline_carrier_del_min_ordinal', 'origin_avg_DEP_DEL15', 'origin_avg_DEP_DELAY_NEW', \n",
    "'avg_origin_delay_over15_lastQ', 'origin_del_ordinal', 'origin_del_min_ordinal', 'OD_avg_DEP_DEL15', 'OD_avg_DEP_DELAY_NEW', \n",
    "'avg_OD_delay_over15_lastQ', 'OD_del_ordinal', 'OD_del_min_ordinal']\n",
    "```\n",
    "\n",
    "After these features were created, we ran our Random Forest Feature selection again on all 78 features to determine the important ones. This spreadsheet below details the most important features.\n",
    "\n",
    "[RF Feature Importance Spreadsheet on the Final Dataset](https://docs.google.com/spreadsheets/d/15syDL-6Izk616mz96n6EUm3pqtBMT9rdblULbL2QIjA/edit#gid=552413007)\n",
    "\n",
    "[RF Feature Selection Implementation Notebook](Second_model_RF_custom_CV_and_voting_draft_rumi.ipynb)\n",
    "\n",
    "\n",
    "The rows in green give the tier 1 importance features (accounting for over 0.5% importance, 14 features) and red give tier 2 importance features (accounting for over 0.01% importance, 26 features). The tier 1 features along account for 97.29% of the importance, while the tier 2 features account for 2.66% of the importance. The remaining 38 features only account for 0.05% of the importance. \n",
    "\n",
    "\n",
    "| Tier 1 Feature              | Feature Importance % |\n",
    "|-----------------------------|----------------------|\n",
    "| previous_DEP_DELAY_NEW      | 24.24                |\n",
    "| prior_dep_delayed           | 20.29                |\n",
    "| time_inb_flight_min         | 19.00                |\n",
    "| avg_hourly_delay_24hr       | 8.34                 |\n",
    "| plane_is_here               | 4.76                 |\n",
    "| evening                     | 3.52                 |\n",
    "| avg_OD_dep_del15_24hr       | 3.21                 |\n",
    "| previous_ARR_DELAY_NEW      | 2.53                 |\n",
    "| avg_ori_airport_delay_24hrs | 2.49                 |\n",
    "| avg_carrier_delay_24hrs     | 2.32                 |\n",
    "| prior_arr_delayed           | 2.25                 |\n",
    "| daytime                     | 1.78                 |\n",
    "| avg_ori_DEP_DELAY_NEW_24hr  | 1.65                 |\n",
    "| OD_avg_DEP_DEL15            | 0.91                 |\n",
    "\n",
    "#### Figure 27\n",
    "![Tier 1 Feature Importance](tier1_features.PNG)\n",
    "\n",
    "These tier 2 features only accounted for 2.66% of the importance total, but may have non-linear relationships with other features. We know, for example, that `departed_for_current_aiport` below and `plane_is_here` above are one-hot encodings for the 3 flight arrival scenarios identified. These features may still prove useful for prediction. As might some of the low importance features not included in the next model, but it could be less likely since all 38 have low total importance and we may reduce generalizability by including them.\n",
    "\n",
    "\n",
    "| Tier 2 Feature                    | Feature Importance % |\n",
    "|-----------------------------------|----------------------|\n",
    "| departed_for_current_aiport       | 0.0040               |\n",
    "| OD_del_ordinal                    | 0.0039               |\n",
    "| avg_OD_delay_min_24hr             | 0.0034               |\n",
    "| TMP_0_air_temperature_imp         | 0.0026               |\n",
    "| WND_3_wind_speed_imp              | 0.0018               |\n",
    "| avg_OD_delay_over15_lastQ         | 0.0015               |\n",
    "| airline_carrier_del_min           | 0.0013               |\n",
    "| airline_carrier_del               | 0.0012               |\n",
    "| aw1_mw1_0_thunderstorm            | 0.0011               |\n",
    "| OD_del_min_ordinal                | 0.0011               |\n",
    "| airline_carrier_del_ordinal       | 0.0009               |\n",
    "| VIS_0_visibility_dist_imp         | 0.0006               |\n",
    "| depDelayPageRank_ordinal          | 0.0004               |\n",
    "| aw1_mw1_0_rain_drizzle            | 0.0004               |\n",
    "| DISTANCE                          | 0.0003               |\n",
    "| origin_avg_DEP_DEL15              | 0.0003               |\n",
    "| arrDelayPageRank_ordinal          | 0.0003               |\n",
    "| airline_carrier_del_min_ordinal   | 0.0003               |\n",
    "| CIG_0_sky_ceiling_height_imp      | 0.0002               |\n",
    "| pacific_islands                   | 0.0002               |\n",
    "| flights_sch_Today_DEST            | 0.0002               |\n",
    "| avg_time_inb_flights_carrier_24hr | 0.0001               |\n",
    "| origin_avg_DEP_DELAY_NEW          | 0.0001               |\n",
    "| flights_sch_Today_ORIGIN          | 0.0001               |\n",
    "| DEW_0_dew_pt_temp_imp             | 0.0001               |\n",
    "| aw1_mw1_0_snow                    | 0.0001               |\n",
    "\n",
    "#### Figure 28\n",
    "![Tier 2 Feature Importance](tier2_features.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "924cdadb-f3fe-4ace-b97c-14e9d8e32990",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 5.2.4 Second LR Model with New Features, Thresholding, and Voting\n",
    "\n",
    "[Second LR Model Notebook](Second_model_RF_custom_CV_and_voting_draft_rumi_xgbm_feats.ipynb)\n",
    "\n",
    "We revisited logistic regression after implementing the new features, bringing the total to 78 (this feature set is the same for the other three models to come). These features are:\n",
    "\n",
    "```\n",
    "['mv1_0_sand_dust_near', 'winter', 'previous_ARR_DELAY_NEW', 'airline_carrier_del_min', 'arrDelayPageRank', 'avg_OD_dep_del15_24hr', 'airline_carrier_del', 'CIG_0_sky_ceiling_height_imp', 'DISTANCE', 'aw1_mw1_0_rain_drizzle', 'autumn', 'avg_ori_DEP_DELAY_NEW_24hr', 'aw1_mw1_0_hail_or_ice', 'ORI_elevation', 'evening', 'mv1_0_thunder_rain_near', 'depDelayPageRank', 'aw1_mw1_0_smoke_haze_dust', 'aw1_mw1_0_snow', 'previous_DEP_DELAY_NEW', 'VIS_0_visibility_dist_imp', 'pacific_islands', 'daytime', 'gd1_0_sky_coverage', 'south', 'AA3_1_liquid_precip', 'avg_OD_delay_min_24hr', 'avg_time_inb_flights_carrier_24hr', 'DEW_0_dew_pt_temp_imp', 'TMP_0_air_temperature_imp', 'departed_for_current_aiport', 'prior_arr_delayed', 'arrDelayPageRank_ordinal', 'au2_4_extreme_wind_weather', 'new_england', 'prior_dep_delayed', 'OD_del_ordinal', 'origin_avg_DEP_DELAY_NEW', 'origin_avg_DEP_DEL15', 'SLP_0_avg_station_press_imp', 'GD1_3_cloud_height_imp', 'flights_sch_Today_DEST', 'southwest', 'spring', 'flights_sch_Today_ORIGIN', 'AA1_1_liquid_precip', 'OD_avg_DEP_DELAY_NEW', 'avg_time_inb_flights_dest_24hr', 'depDelayPageRank_ordinal', 'plane_is_here', 'airline_carrier_del_min_ordinal', 'avg_carrier_delay_24hrs', 'aw1_mw1_0_tornado', 'airline_carrier_del_ordinal', 'avg_carrier_delay_over15_lastQ', 'origin_del_min_ordinal', 'WND_3_wind_speed_imp', 'midwest', 'DEST_elevation', 'origin_del_ordinal', 'avg_OD_delay_over15_lastQ', 'weekend_or_holiday', 'OD_del_min_ordinal', 'mid_atlantic', 'OC1_0_wind_gust_spd_rate_imp', 'aw1_mw1_0_fog', 'west', 'OD_avg_DEP_DEL15', 'time_inb_flight_min', 'avg_ori_airport_delay_24hrs', 'avg_origin_delay_over15_lastQ', 'aw1_mw1_0_freezing_rain_drizzle', 'MA1_0_altimeter_set_rate_imp', 'avg_OD_num_flights_24hr', 'avg_time_inb_flights_origin_24hr', 'MA1_2_station_pres_rate_imp', 'aw1_mw1_0_thunderstorm', 'avg_hourly_delay_24hr']\n",
    "```\n",
    "\n",
    "Although the implementation of logistic regression we used does include options to input the threshold (and could therefore have been tuned along with the other hyperparameters), we instead chose to use the same framework we used with the other models both in order to stay consistent between models and to streamline the process so that it was the same for each model. Additionally, whereas tuning threshold as a hyperparameter would require training new models, using our other inference framework required only computing several new f1 scores, which was less computationally costly.\n",
    "\n",
    "### Thresholding\n",
    "\n",
    "[Thresholding/Voting Notebook](thresholding_and_voting_LR.ipynb)\n",
    "\n",
    "After getting our best results from each fold, we first tuned each fold's threshold to maximize its f1 score (by default, this threshold is 0.5). Our process was as follows:\n",
    "\n",
    "1. Isolate the raw probabilities which led to each prediction\n",
    "2. Compute the predicted labels and f1 score across a wide range of thresholds (0.4 to 0.8)\n",
    "3. \"Zoom in\" on the best-performing threshold to get more precision.\n",
    "4. Repeat until sufficient precision is achieved.\n",
    "5. Re-determine the predictions for the test data using these thresholds from the validation data.\n",
    "\n",
    "#### Results\n",
    "\n",
    "After applying the threshold determined from each fold's validation set to the test data and re-determining labels based on it, we saw a modest improvement across all folds:\n",
    "\n",
    "![LR Folds](LR_thresholding.png)\n",
    "\n",
    "### Voting\n",
    "\n",
    "When each fold's test predictions had been adjusted according to its best threshold from the validation data, the last step was to use a voting system between the folds to make the final prediction of a delay. This had several steps:\n",
    "\n",
    "1. Align the (threshold-adjusted) predictions for each fold with the true label\n",
    "2. Apply weighting to each vote (we weighted based on recency as shown below--we wanted some recency bias but not very much)\n",
    "3. Determine the final prediction with a threshold of 0.5\n",
    "\n",
    "Here again are the folds for reference (training in red, validation in green)\n",
    "\n",
    "#### Figure 29\n",
    "\n",
    "![CV Folds](cv_folds_2.png)\n",
    "\n",
    "### Example with dummy data:\n",
    "\n",
    "| Fold 1 Pred     \t| Fold 2 Pred     \t| Fold 3 Pred     \t| Fold 4 Pred     \t| Fold 5 Pred     \t| Weighted Pred \t| Final Prediction \t|\n",
    "|-----------------\t|-----------------\t|-----------------\t|-----------------\t|-----------------\t|---------------\t|--------------\t|\n",
    "| **Weight 0.15** \t| **Weight 0.15** \t| **Weight 0.2** \t| **Weight 0.3** \t| **Weight 0.2** \t|               \t|              \t|\n",
    "| 0               \t| 0               \t| 0               \t| 1               \t| 1               \t| 0.5           \t| 1            \t|\n",
    "| 1               \t| 1               \t| 1               \t| 0               \t| 0               \t| 0.5           \t| 1            \t|\n",
    "| 0               \t| 0               \t| 0               \t| 1               \t| 0               \t| 0.3           \t| 0            \t|\n",
    "| 1               \t| 1               \t| 0               \t| 1               \t| 1               \t| 0.8           \t| 1            \t|\n",
    "| 1               \t| 1               \t| 1               \t| 1               \t| 1               \t| 1             \t| 1            \t|\n",
    "| 1               \t| 0               \t| 0               \t| 0               \t| 0               \t| 0.15          \t| 0            \t|\n",
    "\n",
    "### Performance\n",
    "\n",
    "After adding these additional features and performing thresholding/voting, the model did see improved performance over the baseline, but not to a tremendous extent:\n",
    "\n",
    "| maxIter | regParam | elasticNetParam | desired_neg_to_pos_ratio | Accuracy | Recall | Precision | f1_score |\n",
    "|---------|----------|-----------------|--------------------------|----------|--------|-----------|----------|\n",
    "| 10      | 0.001    | 0.5             | 1.6                      | 0.8229   | 0.5058 | 0.4965    | 0.5011   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "86426c1e-bfd0-458c-bc19-cf9162524042",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 5.3: Random Forests\n",
    "\n",
    "Random forest is an algorithm that builds a \"forest\" from an ensemble of decision trees. It is able to handle both linear and nonlinear relationships, and it does not require normalization of input features. As is discussed in the 2. Question formulation, random forest has been one of the established machine learning algorithms in flight delay prediction, therefore it can be explained as an essential algorithms, and also the appropriate algorithm to choose as a first tree model to apply.   \n",
    "\n",
    "With the initial feature set, before any tuning, we achieved these results:\n",
    "\n",
    "| maxDepth | maxBins | minInstancesPerNode | numTrees | desired_neg_to_pos_ratio | Accuracy | Recall | Precision | f1_score |\n",
    "|----------|---------|---------------------|----------|--------------------------|----------|--------|-----------|----------|\n",
    "| 6        | 64      | 100                 | 20       | 1.6                      | 0.8305   | 0.4207 | 0.5433    | 0.4730   |\n",
    "\n",
    "## Chosen Hyperparameters\n",
    "\n",
    "- `maxDepth` determines the maximum allowable depth a tree can grow to (number of vertical layers)\n",
    "- `maxBins` determines the maximum number of bins used for splitting features\n",
    "- `minInstancesPerNode` determines the minimum number of training instances a child node must receive in order to continue splitting a node\n",
    "- `numTrees` sets the number of trees in the random forest. \n",
    "\n",
    "Based on some early testing, we set these parameters as follows (with additional time, we will continue to refine these parameters):\n",
    "\n",
    "| maxDepth \t| maxBins \t| minInstancesPerNode \t| numTrees \t|\n",
    "|----------\t|---------\t|---------------------\t|----------\t|\n",
    "| 8        \t| 64      \t| 10                  \t| 100      \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a4edb0b3-c3a7-48cc-8553-3717a1d78d6d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 5.3.1 Thresholding and Voting\n",
    "\n",
    "[Thresholding/Voting Notebook](thresholding_and_voting_RF.ipynb)\n",
    "\n",
    "We used the same procedure as described in 5.2.4 to determine the thresholds for each fold (using the validation set) to optimize f1 score. Then this threshold was applied to the test data to re-determine labels.\n",
    "\n",
    "#### Thresholding Results\n",
    "\n",
    "![RF Thresholding](RF_thresholding.png)\n",
    "\n",
    "We applied the same voting procedure as described above, weighting the more recent folds slightly more heavily to avoid staleness of old data. In this case, we again saw improvements across all folds, and these tended to be larger with RF than they were in LR.\n",
    "\n",
    "#### Voting Results\n",
    "\n",
    "| maxDepth | maxBins | minInstancesPerNode | numTrees | desired_neg_to_pos_ratio | Accuracy | Recall | Precision | f1_score |\n",
    "|----------|---------|---------------------|----------|--------------------------|----------|--------|-----------|----------|\n",
    "| 8        | 64      | 10                 | 100       | 1.6                      | 0.8229   | 0.5188 | 0.5256    | 0.5220   |\n",
    "\n",
    "\n",
    "The results of the vote are fairly good, though they do not outperform every fold. However, because we have decided that we will follow consistent thresholding and voting procedures across all models, we will continue with these results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c38684b8-6fac-470b-8d77-b2b4b7d3c742",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 5.4: XGBoost\n",
    "\n",
    "For our second model, we decided to use [XGBoost](https://xgboost.readthedocs.io/en/stable/) (\"Extreme Gradient Boosting\") to improve performance over our baseline model.\n",
    "\n",
    "[XGBoost Notebook](xgboost_testing_and_tuning.ipynb)\n",
    "\n",
    "XGboost is an optimized, distributed library which implements a version of gradient boosted decision trees in which many decision trees are combined in a way that improves performance. Among other benefits, it is able to capture non-linear relationships between the features and our prediction target, and as with random forests, no normalization of input features is required.\n",
    "\n",
    "## Search Space of Hyperparameters\n",
    "\n",
    "- `eta` is another term for learning rate. It determines how much to reduce the size of the step made in each iteration to help ensure convergence.\n",
    "- `gamma` defines the minimum loss reduction required to make an additional partition on a leaf node. It also helps prevent overfitting.\n",
    "- `max_depth`, as the name suggests, defines the maximum depth of a tree. At high values, models can tend to overfit.\n",
    "- `subsample` defines the proportion of the rows which XGBoost will sample with each boosting iteration. It helps prevent overfitting.\n",
    "- `lambda` is the L2 regularization term which helps the model generalize better to unseen data.\n",
    "- `num_parallel_tree` determines how many parallel trees will be built in each iteration. The results from several trees are combined to form a boosted random forest.\n",
    "- `base_score` is a global measure of bias that determines the initial prediction score.\n",
    "\n",
    "## Additional Options Chosen:\n",
    "- `eval_metric` determines how the tree's performance is scored against the validation set. Because we chose f1 score as our primary target to improve, we chose `aucpr` (area under the precision-recall curve), which is especially well-suited both for unbalanced data sets like this one and for binary classification. Because it takes both recall and precision into account, it will produce results which will nearly already optimize for the f1 score.\n",
    "- `objective` determines the learning objective for the task. This varies by specific application, and for our purposes of binary classification, we chose `binary:logitraw`, which gives us access to the score, predicted label, and probability.\n",
    "- `early_stopping_round` lets the task end early if performance is not improved over several iterations and saves processing time. We set this to `10`.\n",
    "\n",
    "## Hyperparameter Search Process\n",
    "\n",
    "- We started by doing a random search across the hyperparameters above within each fold for several iterations.\n",
    "    - Because training time was somewhat limited, we examined similarities between hyperparameters after the first 10 training iterations with each fold.\n",
    "    - We noticed several folds' hyperparameters started to converge in similar ranges, so to improve our tuning, we took our initial wide brackets and \"zoomed in\" on the areas where several folds had achieved good f1 scores.\n",
    "      - For example, we started searching for `eta` values between 0 and 1, but several models had achieved their best results in the ~0.2 to 0.4 range, so we focused on this range in future searches for `eta`. This \"zooming\" was conducted across all hyperparameters and folds where we saw patterns.\n",
    "\n",
    "- After tuning the models, here were our best-performing hyperparameters for each fold on the validation set\n",
    "- As a reminder, here were our five CV folds:\n",
    "\n",
    "## Figure 30\n",
    "![Custom CV](Screen_Shot_2022_04_10_at_5_14_41.png)\n",
    "\n",
    "## Parameters from Random Search\n",
    "\n",
    "| Fold # \t| eta   \t| gamma \t| max_depth \t| subsample \t| lambda \t| num_parallel_tree \t| base_score \t| Accuracy \t| Recall \t| Precision \t| F1 Score \t|\n",
    "|--------\t|-------\t|-------\t|-----------\t|-----------\t|--------\t|-------------------\t|------------\t|----------\t|--------\t|-----------\t|----------\t|\n",
    "| Fold 1 \t| 0.21  \t| 0.005 \t| 8         \t| 0.565     \t| 1.43   \t| 4                 \t| 0.66       \t| 0.846    \t| 0.555  \t| 0.514     \t| 0.546    \t|\n",
    "| Fold 2 \t| 0.196 \t| 0.288 \t| 4         \t| 0.587     \t| 1.205  \t| 7                 \t| 0.634      \t| 0.8297   \t| 0.527  \t| 0.565     \t| 0.543    \t|\n",
    "| Fold 3 \t| 0.214 \t| 0.229 \t| 4         \t| 0.659     \t| 2.235  \t| 5                 \t| 0.562      \t| 0.831    \t| 0.538  \t| 0.549     \t| 0.544    \t|\n",
    "| Fold 4 \t| 0.218 \t| 0.423 \t| 4         \t| 0.522     \t| 2.28   \t| 6                 \t| 0.595      \t| 0.834    \t| 0.549  \t| 0.543     \t| 0.546    \t|\n",
    "| Fold 5 \t| 0.434 \t| 0.34  \t| 4         \t| 0.995     \t| 1.711  \t| 7                 \t| 0.416      \t| 0.839    \t| 0.545  \t| 0.532     \t| 0.544    \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f9b0ba7e-0901-46ca-ac28-98394c04c574",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 5.4.1: Thresholding and Voting\n",
    "\n",
    "[Thresholding/Voting Notebook](thresholding_and_voting_xgboost.ipynb)\n",
    "\n",
    "We used the same procedure as described in 5.2.4 to determine the thresholds for each fold (using the validation set) to optimize f1 score. Then this threshold was applied to the test data to re-determine labels.\n",
    "\n",
    "#### Thresholding Results\n",
    "\n",
    "In 4/5 cases, we did see modestly improved f1 scores on the set using the threshold optimized using the validation set. For consistency's sake, we will stay with these new labels moving forward.\n",
    "\n",
    "![Thresholding Results](thresholding_results.png)\n",
    "\n",
    "We then applied the same voting procedure as described above, weighting the more recent folds slightly more heavily to avoid staleness of old data. In this case, we again saw improvements across all folds, and these tended to be larger with RF than they were in LR.\n",
    "\n",
    "#### Voting Results\n",
    "\n",
    "![Final Results](final_vote_2.png)\n",
    "\n",
    "| Accuracy | Recall | Precision | f1_score |\n",
    "|----------|--------|-----------|----------|\n",
    "| 0.8305   | 0.5496 | 0.5452    | 0.5474   |\n",
    "\n",
    "We see that the performance of the weighted vote by each fold is better than any individual fold. This is welcome news and suggests that we have been able to compensate for some variance within each of the folds to reach a solution which generalizes better to this unseen data.\n",
    "\n",
    "#### Limited Features Model\n",
    "\n",
    "[Associated Notebook for XGBoost with T1 & T2 Features](xgboost_testing_and_tuning_rumi_2.ipynb)\n",
    "\n",
    "We ran our XGBoost model, and optimized a threshold, for just our tier 1 and tier 2 features to produce the following results. This performed better than using full features for XGBoost.\n",
    "\n",
    "| Accuracy | Recall | Precision | f1_score |\n",
    "|----------|--------|-----------|----------|\n",
    "| 0.8327   | 0.5518 | 0.5518    | 0.5500   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fb1598f6-2abd-4e16-8844-9f1f5a792ce2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 5.5: Gradient Boosted Trees\n",
    "\n",
    "Associated Notebook: [GBT_for Custom_CV_and_Voting](Fourth_model_GBT_custom_CV_and_voting_draft.ipynb)\n",
    "\n",
    "Gradient boosted trees is an algorithm that builds a set of decision trees that are trained sequentially to perform better on the misclassified records of the previous decision tree, and a final prediction is determined through a vote among all trees using weights from each tree's individual performance. XGBoost is similar to this original gradient boosted trees algorithm, but it has additional tuning capabilities, especially on the side of boosting. This difference warranted an investigation into the effectiveness of just the simpler Gradient Boosted Trees algorithm for our use case. This algorithm is able to handle both linear and nonlinear relationships, and it does not require normalization of input features. The tuning parameters of a gradient boosted tree can be grouped into 3 categories ([Guide to Parameter Tuning in GBMs](https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/)):\n",
    "\n",
    "\n",
    "1. Tree-Specific Parameters: These affect each individual tree in the model.\n",
    "2. Boosting Parameters: These affect the boosting operation in the model.\n",
    "3. Miscellaneous Parameters: Other parameters for overall functioning. \n",
    "\n",
    "XGBoost in Section 5.4 explored tuning all of these parameter groups, but our investigation of GBT focused, currently, on just inputing Tree-Specific Parameters since these were best for quick changes in performance for initial investigation and proto-typing. The loss type of our GBT defaulted to logistic loss, with a shrinking contribution of each estimator i.e. stepSize was defaulted to 0.1. The impurity metric for information gain was variance. \n",
    "\n",
    "Compared to Random Forests, [GBTs may perform better for shallower trees (3 < maxDepth < 9)](https://towardsdatascience.com/why-do-random-forest-and-gradient-boosted-decision-trees-have-vastly-different-optimal-max-depth-a64c2f63e127#:~:text=Shallow%20trees%20can%20only%20capture,low%20bias%20and%20high%20variance) but this would still need to be tuned using cross validation hypertuning in the future.\n",
    "\n",
    "## Chosen Hyperparameters\n",
    "\n",
    "- `maxDepth` determines the maximum allowable depth a tree can grow to (number of vertical layers).\n",
    "- `maxBins` determines the maximum number of bins used for splitting continuous features.\n",
    "- `minInstancesPerNode` determines the minimum number of training instances a child node must receive in order to continue splitting a node in the tree.\n",
    "- `maxIter` sets the number of trees in the Gradient Boosted Trees model. \n",
    "\n",
    "## Other Parameters \n",
    "- `threshold` gives the probability threshold to classify a positive class/flight delay (Tuned in Section 5.5.1).\n",
    "\n",
    "Our investigation into GBMs is recent, so we will continue to refine hyperparameters with additional time. However, based on some brief early testing, we set these parameters as follows:\n",
    "\n",
    "| maxDepth \t| maxBins \t| minInstancesPerNode \t| numTrees \t|\n",
    "|----------\t|---------\t|---------------------\t|----------\t|\n",
    "| 5        \t| 64      \t| 10                  \t| 100      \t|\n",
    "\n",
    "## Results\n",
    "\n",
    "\n",
    "| CV Model | maxDepth | maxBins | minInstancesPerNode | maxIter | Accuracy | Recall | Precision | F1 measure |\n",
    "|----------|----------|---------|---------------------|---------|----------|--------|-----------|------------|\n",
    "| 1        | 5        | 64      | 10                  | 100     | 0.835    | 0.518  | 0.563     | 0.540      |\n",
    "| 2        | 5        | 64      | 10                  | 100     | 0.825    | 0.555  | 0.530     | 0.542      |\n",
    "| 3        | 5        | 64      | 10                  | 100     | 0.829    | 0.541  | 0.541     | 0.541      |\n",
    "| 4        | 5        | 64      | 10                  | 100     | 0.829    | 0.545  | 0.541     | 0.543      |\n",
    "| 5        | 5        | 64      | 10                  | 100     | 0.834    | 0.527  | 0.557     | 0.542      |\n",
    "\n",
    "\n",
    "We developed 5 models for each CV fold and provided their results on the 2019 dataset, without thresholding. Thresholding will be done using each CV fold's validation sets, then 2019 test data f1 scores will finally be computed. We will vote among all 5 models, using weights for staleness just as we did for the tree models above, to compute a final prediction and f1 score for 2019 test data.\n",
    "\n",
    "## Trade Offs\n",
    "\n",
    "Gradient boosted models (GBT, XGBoost) yielded the highest f1 score across our models but also took the longest the train. This could be a consequence of sequential tree development. GBTs also have many hyperparameters to tune, exceeding random forests, since there are also boosting parameters. Lastly, GBTs are difficult to explain since they are an ensemble of decision trees, which normally are easy to explain through the splitting features at nodes. Ensembles, boosting, and weighting diminishes the ease of explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "574c07fe-211e-4cfc-b68c-fff8e56ed5eb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 5.5.1: Thresholding and Voting\n",
    "\n",
    "[Thresholding/Voting Notebook](thresholding_and_voting_GBT.ipynb)\n",
    "\n",
    "We used the same procedure as described in 5.2.4 to determine the thresholds for each fold (using the validation set) to optimize f1 score. Then this threshold was applied to the test data to re-determine labels.\n",
    "\n",
    "#### Thresholding Results\n",
    "\n",
    "This time around, we saw improvements from thresholding in only 2/5 folds. However, we made the decision to use the same thresholding procedure across all models, so we will continue with these labels.\n",
    "\n",
    "![GBT Thresholding](GBT_thresholding.png)\n",
    "\n",
    "We then applied the same voting procedure as described above, weighting the more recent folds slightly more heavily to avoid staleness of old data. In this case, we again saw improvements across all folds, and these tended to be larger with RF than they were in LR.\n",
    "\n",
    "#### Voting Results\n",
    "\n",
    "![GBT Vote Results](GBT_vote_results.png)\n",
    "\n",
    "| Accuracy | Recall | Precision | f1_score |\n",
    "|----------|--------|-----------|----------|\n",
    "| 0.8260   | 0.5572 | 0.5320    | 0.5443   |\n",
    "\n",
    "As before, we again see that the performance of the weighted vote by each fold is better than any individual fold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9d954a98-0213-43ca-b461-63d14712a21e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 5.6: Voting Ensemble of All Four Models\n",
    "\n",
    "[Ensemble Voting Notebook](ensemble_voting.ipynb)\n",
    "\n",
    "We created a voting ensemble of our 4 best models of each type: Logistic Regression, Random Forest, XGBoost, and Gradient Boosted Trees.\n",
    "\n",
    "In order to do this, we first imported and aligned the final (thresholded/voted) labels from each of the four models. In order to avoid ties and give stronger models slightly more say, we weighted each model's score by its final f1 score, then took the weighted average.\n",
    "\n",
    "If the weighted average was 0.5 or greater, the result of the vote was a delay (1). Otherwise, the vote did not pass (no delay or 0).\n",
    "\n",
    "Here is a subset of the assembled dataframe which shows the structure:\n",
    "\n",
    "| label \t| XGB_pred \t| GBT_pred \t| RF_pred \t| LR_pred \t| weighted_pred \t|\n",
    "|-------\t|----------\t|----------\t|---------\t|---------\t|---------------\t|\n",
    "| 0     \t| 0        \t| 0        \t| 1       \t| 1       \t| 0             \t|\n",
    "| 1     \t| 1        \t| 1        \t| 0       \t| 1       \t| 1             \t|\n",
    "| 0     \t| 0        \t| 0        \t| 1       \t| 0       \t| 0             \t|\n",
    "| 1     \t| 1        \t| 1        \t| 0       \t| 0       \t| 1             \t|\n",
    "| 0     \t| 1        \t| 1        \t| 1       \t| 0       \t| 1             \t|\n",
    "| 1     \t| 1        \t| 1        \t| 1       \t| 1       \t| 1             \t|\n",
    "\n",
    "### Voting Ensemble Results\n",
    "\n",
    "| Accuracy | Recall | Precision | f1_score |\n",
    "|----------|--------|-----------|----------|\n",
    "| 0.8298   | 0.5480 | 0.5434    | 0.5457   |\n",
    "\n",
    "We were disappointed to discover that taking a weighted average of thresholded/voted results from each model did not result in improved performance. We believe it was a novel idea, but in a sense, we do believe it means we are extracting most of the meaning that we can out of the features we have engineered. In order to further improve performance, more features would likely need to be engineered, or we might need to turn to a deep learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8deaeefa-9a37-4bbb-aef7-e62fc34d7924",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 6. Algorithm Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "77657e03-6e07-4e52-8782-d9a4a509c9a1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As we stated above, our baseline model was logistic regression. The following notebook goes more into how logistic regression works and what happens under the hood. \n",
    "\n",
    "[Logistic Regression RDD Implementation Notebook](Logistic_regression.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e3f53154-6d2f-4331-8882-c3e4d6b1f1d4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 7. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7c3e05af-0bce-4f0e-84f3-965bc80ac347",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Summary Table of Models\n",
    "\n",
    "The following provides a summary table comparing the performance of all of our models investigated so far.\n",
    "\n",
    "| Model   Name                          | Section | Number of Features | Internal Weighted Voting | F1 Score |\n",
    "|---------------------------------------|---------|--------------------|--------------------------|----------|\n",
    "| Logistic   Regression (Best Baseline) | 5.2.2   | 49                 | No                       | 0.473    |\n",
    "| Logistic   Regression Final           | 5.2.4   | 78                 | Yes                      | 0.501    |\n",
    "| Random   Forest                       | 5.3.1   | 78                 | Yes                      | 0.522    |\n",
    "| XGBoost                               | 5.4.1   | 78                 | Yes                      | 0.547   |\n",
    "| XGBoost                               | 5.4.1   | 46                 | Yes                      | 0.550   |\n",
    "| Gradient   Boosted Trees              | 5.5.1   | 78                 | Yes                      | 0.544   |\n",
    "| 4 Model   Voting Ensemble             | 5.6     | 78                 | Yes                      | 0.546   |\n",
    "\n",
    "Our best model was our highly tuned XGBoost with an F1 Score of 0.550 (using tier 1 & tier 2 features only), our second best was our 4 Model Voting ensemble with an F1 score of 0.546, but our minimally tuned Gradient Boosted Trees was very close behind with and F1 score of 0.544. Perhaps more extensive hyperparameter tuning on the Gradient Boosted Model might yield better results; but we will leave this investigation for our next steps. Additional Tuning could also benefit Random Forests, which achieved a 0.522 with minimal tuning as well. However, our analysis seems to hint that Gradient Boosted models yielded better performance for our problem, and undersampled data, at the cost of extra training times.\n",
    "\n",
    "## Performance of Novel Approaches\n",
    "\n",
    "We tried two novel approaches for inference on our models. The first was **Internal Weighted Voting**, which trained a model for each Cross Validation fold, tuned on its held out set, and preserved all 5 models to take a weighted vote among them for the 2019 test set and any future unseen data. The next novel approach was the **4 Model Voting Ensemble**, which took each of these models with Internal Weighted Voting (Logistic Regression Final, Random Forest, XGBoost, and Gradient Boosted Trees) and ran a prediction through them all to take a vote for the final prediction. \n",
    "\n",
    "Firstly **Internal Weighted Voting** saw increased performance over any individual XGBoost or GBT model made on the folds, but it did not improve over all models for Logistic Regression and Random Forests. Additonally, this method, which tunes 5 CV models individually, is faster to train than trying the same set of hyperparameters across all 5 models since each model and then training 1 model on all of the training data using the optimal parameters found. The cost of this method was preserving 5 sub-models for each model to use for inference on unseen data. The weights of the voting system cannot be tuned without having another cross validation set left out of all 5 models. That required us to propose weighting these irregular rolling CV folds, and the models they produced, based on staleness. Running the same number of predictions through each of 5 models, rather than one model, and joining together to take a vote is a more complicated process. To assess if this approach is better, we should aquire the computing resources to train our tree models using random search hyperparameter tuning with the same values on each and then use these values to produce a model trained on all of the training data (similar to our Baseline Approach). Comparing the 2019 test performance of these model against our Internal Weighted Voting versions would reveal which approach was definitively better.\n",
    "\n",
    "Secondly the **4 Model Voting Ensemble** did not improve performance over our best model XGBoost - it perfomed just about the same. We hoped that the voting ensemble of these models would provide additional prediction power for test examples where some models balanced out the mispredictions of others. This did not seem to be the case. The implementation of this system required joining the predictions of all models together and determining the majority vote. In practice, this would require having the best of each model ready to pass a prediction value through. This is scalable. Possible areas of future improvement for this approach are, of course, improving the individual 4 models themselves through additional hyperparameter tuning and possible feature engineering, but also feeding in the raw probabilities predicted or the predictions themselves into a more complex model. Rather than simple voting, this 4 model ensemble could be a logistic regression model. The trade off of this approach, and why we did not use it here, was that we can only train that logistic regression final inference model on non-test data. This would require each model to save off its training predictions across the CV splits and the ensemble logistic regression model to be trained as follows: inputs are CV training predictions for each of the 4 models and the labels are if those flights were delayed. This method is quite a lot of data handling, which may be difficult to continously update as we retrain the entire ensembled system. Nevertheless, it is a direction we would like to investigate after improving our 4 models first.\n",
    "\n",
    "## Final Recommendations\n",
    "\n",
    "The XGBoost model produced provides a solution for predicting flight delays that balances false negatives and false positives well, since the precision and recall are close in value. Gradient boosted trees also show promise of rivaling or exceeding the XGBoost model, with Random Forests slightly behind. The ease of training logistic regression helps understand if improvements are made through feature engineering and it never hurts to quickly train and have that model ready for possible ventures again into 4 model ensemble methods. The XGBoost model and GBT model certainly have room to grow and be tuned, which will be the primary focus of our next phase of development for our app, but the XGBoost model now can provide a potentially helpful solution for a limited preliminary deployment. We recommend the client to assess the potential use of our current model, with an improved version released in the near future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "38f0ea6b-8e80-4421-90d3-3f08e6d585df",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 8. Application of Course Concepts\n",
    "\n",
    "\n",
    "Scalability and time complexity issues were frustrating and came up when we were trying to do the join and run different models. It would take hours to run something and after the cluster size was reduced, we had to be even more strategic. One strategy was to save multiple tables to blob storage so that we would not need to rerun cells. \n",
    "\n",
    "We attempted one-hot encoding some categorical weather variables in order to improve the performance of the random forest model, but ultimately found it was unfeasible because there were more than 20 values in some categorical variables. There were many opportunities for data leakage errors because we were working with a time series. For example, we sometimes wanted to use averages to impute the missing values. However, for flight data we had to use the average from a previous time window so that the average was not computed with data from after when the missing data would have been observed. We actually ended up importing data from the end of 2014 to use for imputing missing values for the first half of 2015. \n",
    "\n",
    "We also had to be wary of data leakage when feature engineering new variables from the flight data. Particularly since we could not use data less than two hours before the flight was scheduled to depart to predict its delay and thus had to be very cognizant of the time windows we used for feature engineering.\n",
    "\n",
    "Because we were working with binary and numeric variables, we decided to use min/max standardization instead of normalization on the features after splitting them into training and test sets since min max would keep these binary features unchanged. We needed to standardize each fold separately to ensure no data leakage.\n",
    "\n",
    "We considered distributed data structures and parallel computing efficiency by avoiding row wise passes through our dataframes. When dealing with data this large in scale, row wise operations are exceedingly expensive if they cannot be parallelized through spark dataframe or spark sql functionality.\n",
    "\n",
    "We learned that time series data makes the problem much more complicated in order to prevent data leakage. Even using Brieman's method for encoding categorical variables as ordinal, a very useful practice we came to find, was complicated because time-series data needed the averages or aggregation to occur on past data for integrity.\n",
    "\n",
    "Hyperparameter tuning can be an exhaustive and long process. Random search helps explore possible parameters more quickly than grid search, and performs better than greedy search which may bias performance based on the starting features searched upon.\n",
    "Feature engineering is a large part of the task because much of the data, in its raw form, would not necessarily be correlated with the outcome or even be usable due to missing values or being an outcome variable of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2d0d030e-23a2-48d9-b030-14b75326501d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 9. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "782f456d-07e0-4e3c-a693-771a469ffc0e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Dataset\n",
    "- [IATA Dataset](https://www.partow.net/miscellaneous/airportdatabase/) (we explored using this but opted to use OpenFlights instead)\n",
    "- [OpenFlights](https://openflights.org/data.html) (used for airport elevation and timezone)\n",
    "\n",
    "#### Past academic study\n",
    "- [A Review on Flight Delay Prediction](https://arxiv.org/abs/1703.06118)\n",
    "- [Flight Delay Assesement(Airports Comissions: Goverment of the UK)](https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/372619/AC08_tagged.pdf)\n",
    "- [Flight Delay Prediction using Airport Situational Awareness Map](https://arxiv.org/pdf/1911.01605.pdf)\n",
    "\n",
    "#### Articles\n",
    "- [Airline Disruption Management: Driving Customer Satisfaction through Real Time and Personalized Notifications](https://www.igtsolutions.com/blog/airline-disruption-management/)\n",
    "- [Mck Kinsey Report: The Future of CX](https://www.mckinsey.com/business-functions/marketing-and-sales/our-insights/prediction-the-future-of-cx)\n",
    "- [Customer Satisfaction in the Airline Industry](https://blogs.perficient.com/2018/05/14/customer-satisfaction-in-the-airline-industry/)\n",
    "- [Is Delta The Leading US Airline? These Stats Would Say So...](https://simpleflying.com/2019-us-airlines-performance/)\n",
    "- [These are the worst (and best) airlines for 2019, based on mishandled luggage, delays and more](https://www.cnbc.com/2020/01/16/best-and-worst-airlines-for-2019.html)\n",
    "- [The Best and Worst U.S. Airlines of 2019](https://www.wsj.com/articles/the-best-and-worst-u-s-airlines-of-2019-11579097301)\n",
    "- [2019 Airline Scorecard: Rankings of major carriers in key operational areas, best to worst](https://graphics.wsj.com/dynamic-inset-iframer/?url=https://asset.wsj.net/wsjnewsgraphics/data-tables/3c928058-b857-43d3-87db-6c7bd82ecc1e.json)\n",
    "\n",
    "\n",
    "\n",
    "####Cross validations\n",
    "- [Cross-Validation strategies for Time Series forecasting](https://hub.packtpub.com/cross-validation-strategies-for-time-series-forecasting-tutorial/)\n",
    "- [Cross Validation in Time Series](https://medium.com/@soumyachess1496/cross-validation-in-time-series-566ae4981ce4#:~:text=Cross%20Validation%20on%20Time%20Series,for%20the%20forecasted%20data%20points)\n",
    "\n",
    "#### Models\n",
    "- [Logistic Regression](https://www.geeksforgeeks.org/advantages-and-disadvantages-of-logistic-regression/)\n",
    "- [XGBoost Depth](https://towardsdatascience.com/why-do-random-forest-and-gradient-boosted-decision-trees-have-vastly-different-optimal-max-depth-a64c2f63e127#:~:text=Shallow%20trees%20can%20only%20capture,low%20bias%20and%20high%20variance)\n",
    "- [XGBoost Size](https://machinelearningmastery.com/tune-number-size-decision-trees-xgboost-python/)\n",
    "- [XGBoost Reference](https://xgboost.readthedocs.io/en/stable/parameter.html)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Group6_Main_Notebook",
   "notebookOrigID": 1038316885894777,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
